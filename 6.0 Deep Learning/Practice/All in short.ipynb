{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure python 3 forward compatibility\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = np.random.randn(n_output, n_input)\n",
    "        self.b = np.random.randn(n_output, 1)\n",
    "    def output(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return sigmoid(self.W.dot(X) + self.b)\n",
    "\n",
    "class SigmoidNetwork:\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        '''\n",
    "        :parameters:\n",
    "            - layer_sizes : list of int\n",
    "                List of layer sizes of length L+1 (including the input dimensionality)\n",
    "        '''\n",
    "        self.layers = []\n",
    "        for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(SigmoidLayer(n_input, n_output))\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.2):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(1, -1)\n",
    "        \n",
    "        # Forward pass - compute a^n for n in {0, ... L}\n",
    "        layer_outputs = [X]\n",
    "        for layer in self.layers:\n",
    "            layer_outputs.append(layer.output(layer_outputs[-1]))\n",
    "        \n",
    "        # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1}\n",
    "        cost_partials = [layer_outputs[-1] - y]\n",
    "        for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])):\n",
    "            cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output))\n",
    "        cost_partials.reverse()\n",
    "        \n",
    "        # Compute weight gradient step\n",
    "        W_updates = []\n",
    "        for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]):\n",
    "            W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1])\n",
    "        # and biases\n",
    "        b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]]\n",
    "        \n",
    "        for W_update, b_update, layer in zip(W_updates, b_updates, self.layers):\n",
    "            layer.W -= W_update*learning_rate\n",
    "            layer.b -= b_update*learning_rate\n",
    "\n",
    "    def output(self, X):\n",
    "        a = np.array(X)\n",
    "        if a.ndim == 1:\n",
    "            a = a.reshape(-1, 1)\n",
    "        for layer in self.layers:\n",
    "            a = layer.output(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\tOutput\tQuantized\n",
      "[0, 0]\t0.0165\t[0]\n",
      "[1, 0]\t0.4959\t[0]\n",
      "[0, 1]\t0.9895\t[1]\n",
      "[1, 1]\t0.5038\t[1]\n"
     ]
    }
   ],
   "source": [
    "nn = SigmoidNetwork([2, 2, 1])\n",
    "X = np.array([[0, 1, 0, 1], \n",
    "              [0, 0, 1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "for n in range(int(1e3)):\n",
    "    nn.train(X, y, learning_rate=1.)\n",
    "print(\"Input\\tOutput\\tQuantized\")\n",
    "for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n",
    "    print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] > .5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88774d21c2594a3f8cb3b97e0661705c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='beta', max=10.0, min=-1.0), Output()), _dom_classes=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import *\n",
    "import matplotlib.pyplot as plt\n",
    "logistic = lambda h, beta: 1./(1 + np.exp(-beta * h))\n",
    "\n",
    "@interact(beta=(-1, 10, .1))\n",
    "def logistic_plot(beta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    plt.plot(hvals, logistic(hvals, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Eager execution is: True\n",
      "Keras version: 3.3.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Damodar'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damodar = tf.constant(\"Damodar\")\n",
    "damodar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Damodar'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damodar.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damodar = tf.constant(1,dtype=tf.int64)\n",
    "damodar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damo = tf.constant([[1,2],[3,4]])\n",
    "damo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(damo.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damo.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones(shape=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const2 = tf.constant(2,tf.int32)\n",
    "const3 = tf.constant(3,tf.int32)\n",
    "result = tf.add(const2,const3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-0.7423031 ,  0.49730617,  0.56104356],\n",
       "       [ 0.32578176,  0.37715843,  1.0766712 ],\n",
       "       [-1.2800843 ,  0.7142359 ,  0.62109447]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape=(3,3),mean=0,stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[9, 1, 9],\n",
       "       [3, 7, 4],\n",
       "       [5, 6, 0]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform(shape=(3,3),minval=0,maxval=10,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var0 = 24\n",
    "var1 = tf.Variable(42)\n",
    "var2 = tf.Variable([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2, 3) dtype=int32, numpy=\n",
       "array([[[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 7,  8,  9],\n",
       "        [10, 11, 12]]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float64, numpy=89.0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_var = tf.Variable(89,dtype = tf.float64)\n",
    "float_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=89>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = tf.Variable(89)\n",
    "print(var)\n",
    "var.assign(90)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 0.40570402,  1.2468215 ,  1.3619007 ],\n",
      "       [ 0.8154949 ,  0.9904125 ,  0.02853067],\n",
      "       [-0.13177648, -0.11049007, -0.37906578]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "inital_value = tf.random.normal(shape=(3,3))\n",
    "a = tf.Variable(inital_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 2.411582  , -0.42689428,  0.13985828],\n",
      "       [-0.838627  , -0.5892098 ,  1.5254803 ],\n",
      "       [-1.769425  ,  0.5824669 , -0.01330743]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 2.411582  , -0.42689428,  0.13985828],\n",
      "       [-0.838627  , -0.5892098 ,  1.5254803 ],\n",
      "       [-1.769425  ,  0.5824669 , -0.01330743]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "new_value = tf.random.normal(shape=(3,3))\n",
    "a.assign(new_value)\n",
    "print(a)\n",
    "for i in  range(3):\n",
    "    for j in range(3):\n",
    "        assert a[i,j].numpy() == new_value[i,j].numpy()\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 2.298068  , -1.0805799 ,  0.5394056 ],\n",
      "       [-2.0760844 , -0.07380164,  0.9744378 ],\n",
      "       [-1.9742116 ,  1.5502973 ,  2.0029082 ]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 2.298068  , -1.0805799 ,  0.5394056 ],\n",
      "       [-2.0760844 , -0.07380164,  0.9744378 ],\n",
      "       [-1.9742116 ,  1.5502973 ,  2.0029082 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "added_value = tf.random.normal(shape=(3,3))\n",
    "a.assign_add(added_value)\n",
    "print(a)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        assert a[i,j].numpy() == new_value[i,j].numpy() + added_value[i,j].numpy()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# shaping tensor\n",
    "tensor = tf.Variable([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]], shape=(2, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = tf.reshape(tensor,[2,6])\n",
    "print(tensor1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1  2  3  4  5  6  7  8  9 10 11 12]], shape=(1, 12), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor2 = tf.reshape(tensor,[1,12])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking tensor\n",
    "tf.rank(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=9>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor3 = tensor[1,0,2]\n",
    "tensor3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_size = tf.size(input=tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
       "array([[[  1,   4,   9],\n",
       "        [ 16,  25,  36]],\n",
       "\n",
       "       [[ 49,  64,  81],\n",
       "        [100, 121, 144]]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor*tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
       "array([[[ 4,  8, 12],\n",
       "        [16, 20, 24]],\n",
       "\n",
       "       [[28, 32, 36],\n",
       "        [40, 44, 48]]])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_u = tf.constant([[2,5,7],[3,6,8]])\n",
    "matrix_v = tf.constant([[3,6],[4,7],[5,8]])\n",
    "product = tf.matmul(matrix_u,matrix_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 61, 103],\n",
       "       [ 73, 124]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int32'>\n",
      "<dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.dtype)\n",
    "i = tf.cast(tensor1,tf.int64)\n",
    "print(i.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = tf.cast(tf.constant(2.4),dtype=tf.int32)    \n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[9, 7, 8, 0], [5, 6, 0], [1, 2, 3, 4, 0], [4, 5, 6]]>\n",
      "tf.Tensor([9 7 8 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([5 6 0], shape=(3,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 0], shape=(5,), dtype=int32)\n",
      "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ragged = tf.ragged.constant([[9,7,8,0],[5,6,0],[1,2,3,4,0],[4,5,6]])\n",
    "print(ragged)\n",
    "print(ragged[0,:])\n",
    "print(ragged[1,:])\n",
    "print(ragged[2,:])\n",
    "print(ragged[3,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([False False False  True False False False False False False], shape=(10,), dtype=bool)\n",
      "tf.Tensor([ 9  4  1  0  1  4  9 16 25 36], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "varx = tf.Variable([1,2,3,4,5,6,7,8,9,10])\n",
    "vary = 4\n",
    "varz = tf.math.equal(varx,vary)\n",
    "print(varz)\n",
    "varm = tf.math.squared_difference(varx,vary)\n",
    "print(varm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = tf.constant([1,2,3,4,5,6,7,8,9,10])\n",
    "tf.reduce_mean(input_tensor=number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=int32) \n",
      " tf.Tensor(\n",
      "[[4 5 6]\n",
      " [7 8 9]], shape=(2, 3), dtype=int32) \n",
      " tf.Tensor(\n",
      "[[ 2  3  4]\n",
      " [ 8  9 10]], shape=(2, 3), dtype=int32) \n",
      " tf.Tensor([[[6]]], shape=(1, 1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "_3d_tensor = tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "print(tf.reduce_mean(input_tensor=_3d_tensor), \"\\n\",\n",
    "tf.reduce_mean(input_tensor=_3d_tensor,axis=0),'\\n', # 1 + 7 / 2 = 4\n",
    "tf.reduce_mean(input_tensor=_3d_tensor,axis=1),'\\n', # \n",
    "tf.reduce_mean(input_tensor=_3d_tensor,keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[10.638121, 11.965033],\n",
       "       [ 9.186861,  9.235863],\n",
       "       [ 9.319022, 13.770314]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape = (3,2), mean=10, stddev=2, dtype=tf.float32, seed=None, name=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11.123277   7.9276066]\n",
      " [13.963597   9.845302 ]\n",
      " [ 9.547457  10.164621 ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ran = tf.random.normal(shape = (3,2), mean=10.0, stddev=2.0)\n",
    "print(ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4 6]\n",
      " [5 2]\n",
      " [8 8]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[9 7]\n",
      " [9 4]\n",
      " [0 1]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ran1 = tf.random.uniform(shape=(3,2),minval=0,maxval=10,dtype=tf.int32)\n",
    "print(ran1)\n",
    "ran2 = tf.random.uniform(shape=(3,2),minval=0,maxval=10,dtype=tf.int32)\n",
    "print(ran2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4 6]\n",
      " [5 2]\n",
      " [8 8]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[9 7]\n",
      " [9 4]\n",
      " [0 1]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(11)\n",
    "ran1 = tf.random.uniform(shape=(3,2),minval=0,maxval=10,dtype=tf.int32)\n",
    "print(ran1)\n",
    "ran2 = tf.random.uniform(shape=(3,2),minval=0,maxval=10,dtype=tf.int32)\n",
    "print(ran2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice1 = tf.Variable(tf.random.uniform([10,1],minval=1,maxval=7,dtype=tf.int32))\n",
    "dice2 = tf.Variable(tf.random.uniform([10,1],minval=1,maxval=7,dtype=tf.int32))\n",
    "dice_sum = dice1 + dice2\n",
    "resulting_matrix = tf.concat(values=[dice1,dice2,dice_sum],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 3), dtype=int32, numpy=\n",
       "array([[ 5,  5, 10],\n",
       "       [ 4,  3,  7],\n",
       "       [ 5,  3,  8],\n",
       "       [ 3,  3,  6],\n",
       "       [ 1,  4,  5],\n",
       "       [ 4,  1,  5],\n",
       "       [ 5,  1,  6],\n",
       "       [ 6,  4, 10],\n",
       "       [ 3,  3,  6],\n",
       "       [ 2,  3,  5]])>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2 11 23 43  5  6  7  8  9 10], shape=(10,), dtype=int32)\n",
      "index of max value: 3\n",
      "Max value: 43\n",
      "index of min value: 0\n",
      "Min value: 2\n"
     ]
    }
   ],
   "source": [
    "t5 = tf.constant([2,11,23,43,5,6,7,8,9,10])\n",
    "\n",
    "print(t5)\n",
    "\n",
    "i = tf.argmax(input=t5)\n",
    "print('index of max value:',i.numpy())\n",
    "print('Max value:',t5[i].numpy())\n",
    "\n",
    "i = tf.argmin(input=t5)\n",
    "print('index of min value:',i.numpy())\n",
    "print('Min value:',t5[i].numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2 11 23 43  5]\n",
      " [ 6  7  8  9 10]], shape=(2, 5), dtype=int32)\n",
      "Column wise max value: [1 0 0 0 1]\n",
      "Row wise max value: [3 4]\n",
      "Column wise min value: [0 1 1 1 0]\n",
      "Row wise min value: [0 0]\n"
     ]
    }
   ],
   "source": [
    "t5 = tf.constant([2,11,23,43,5,6,7,8,9,10])\n",
    "t6 = tf.reshape(t5,[2,5])\n",
    "print(t6)\n",
    "\n",
    "i = tf.argmax(input=t6,axis=0)\n",
    "print('Column wise max value:',i.numpy())\n",
    "\n",
    "i = tf.argmax(input=t6,axis=1)\n",
    "print('Row wise max value:',i.numpy())\n",
    "\n",
    "i = tf.argmin(input=t6,axis=0)\n",
    "print('Column wise min value:',i.numpy())\n",
    "\n",
    "i = tf.argmin(input=t6,axis=1)\n",
    "print('Row wise min value:',i.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 4) dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0],\n",
      "       [0, 0, 0, 0]])>\n"
     ]
    }
   ],
   "source": [
    "variable = tf.Variable([[1,3,5,7],[11,13,17,19]])\n",
    "checkpoint = tf.train.Checkpoint(var= variable)\n",
    "save_path = checkpoint.save('./vars')\n",
    "variable.assign([[0,0,0,0],[0,0,0,0]])\n",
    "print(variable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 4) dtype=int32, numpy=\n",
       "array([[ 1,  3,  5,  7],\n",
       "       [11, 13, 17, 19]])>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(save_path)\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 109.0\n",
      "f2: 109.0\n"
     ]
    }
   ],
   "source": [
    "def f1(x,y):\n",
    "\n",
    "    return tf.reduce_mean(input_tensor=tf.multiply(x**2,5) + y**2)\n",
    "\n",
    "f2 = tf.function(f1)\n",
    "x = tf.constant([4.0,-5.0])\n",
    "y = tf.constant([2.0,3.0])\n",
    "print('f1:',f1(x,y).numpy())\n",
    "print('f2:',f2(x,y).numpy())\n",
    "assert f1(x,y).numpy() == f2(x,y).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.469929    0.89920384]\n",
      " [-0.66446567 -0.79767007]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2,2))\n",
    "b = tf.random.normal(shape=(2,2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c,a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.28425562, -0.49051556],\n",
      "       [ 0.7645078 ,  0.41764784]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[-0.1684204 -0.5020665]\n",
      " [ 0.9471321  0.5898169]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(a)\n",
    "print(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c,a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.57568985 0.7655419 ]\n",
      " [0.12753117 0.9209411 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c,a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da,a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([0,1,2,3,4])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[0,1,2],[3,4,5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(t1.dtype)\n",
    "print(t2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a tensor without gradient computation\n",
    "x = torch.tensor([1, 2, 3],dtype=torch.float32)\n",
    "\n",
    "# Check if gradient computation is enabled\n",
    "print(x.requires_grad)  # Output: False\n",
    "\n",
    "# Enable gradient computation for the tensor\n",
    "x.requires_grad_(True)\n",
    "\n",
    "# Check if gradient computation is enabled after modification\n",
    "print(x.requires_grad)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4.,requires_grad=True)\n",
    "b = torch.tensor(5.,requires_grad=True)\n",
    "x,w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = w*x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print('dy/dx:',x.grad)\n",
    "print('dy/dw:',w.grad)\n",
    "print('dy/db:',b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t6 = torch.full((3,2),7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 7],\n",
       "        [7, 7],\n",
       "        [7, 7]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2],[3,4]],dtype=torch.float32)\n",
    "t2 = torch.tensor([[9,8],[7,6]],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 8.],\n",
      "        [7., 6.],\n",
      "        [1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9., 8., 1., 2.],\n",
       "        [7., 6., 3., 4.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.cat((t2,t1),dim=0)\n",
    "print(t3)\n",
    "t4 = torch.cat((t2,t1),dim=1)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8415,  0.9093],\n",
       "        [ 0.1411, -0.7568]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1 = torch.sin(t1)\n",
    "tt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73,67,43],[91,88,64],[87,134,58],[102,43,37],[69,96,70]],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56,70],[81,101],[119,133],[22,37],[103,119]],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "\n",
    "print(inputs) \n",
    "\"\\n\"\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2030, -1.1453,  0.9079],\n",
      "        [-1.2951, -0.2417, -1.0556]], requires_grad=True)\n",
      "tensor([-1.0601,  0.2237], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2,3,requires_grad=True)\n",
    "b = torch.randn(2,requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ ( w.t()) +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[109.8263, 108.0341],\n",
      "        [147.7135, 138.9620],\n",
      "        [173.3903, 178.3041],\n",
      "        [103.1631, 105.4401],\n",
      "        [146.7207, 131.6104]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(actual,targets):\n",
    "    diff = actual - targets\n",
    "    return torch.sum(diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2858.8459, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4863,  0.6913,  0.6960],\n",
      "        [ 0.6636,  0.9119, -0.0080]], requires_grad=True) \n",
      " tensor([[5205.5161, 4890.5332, 3160.4651],\n",
      "        [3604.6975, 3222.6401, 2021.5354]])\n"
     ]
    }
   ],
   "source": [
    "print(w, '\\n', w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9199, -1.1653], requires_grad=True) \n",
      " tensor([59.9628, 40.4701])\n"
     ]
    }
   ],
   "source": [
    "print(b, '\\n', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -23.9364, -155.9016],\n",
      "        [ -25.2677, -206.4559],\n",
      "        [ -84.2114, -206.0584],\n",
      "        [   3.9906, -181.3268],\n",
      "        [ -33.4487, -186.2301]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(47927.3398, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -8887.4072, -11162.2217,  -6448.4844],\n",
      "        [-23459.5195, -25263.0977, -15700.1055]]) \n",
      "\n",
      "tensor([-108.7747, -279.1946])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w.grad, \"\\n\")\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2919, -1.0337,  0.9724],\n",
      "        [-1.0605,  0.0110, -0.8986]], requires_grad=True) \n",
      " tensor([-1.0590,  0.2265], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w, '\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32627.7559, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = MSE(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0/400 and loss 0.5093007683753967\n",
      "epochs 1/400 and loss 0.5092987418174744\n",
      "epochs 2/400 and loss 0.5092974901199341\n",
      "epochs 3/400 and loss 0.5092979073524475\n",
      "epochs 4/400 and loss 0.5092969536781311\n",
      "epochs 5/400 and loss 0.5092988610267639\n",
      "epochs 6/400 and loss 0.5092990398406982\n",
      "epochs 7/400 and loss 0.5092988610267639\n",
      "epochs 8/400 and loss 0.509297251701355\n",
      "epochs 9/400 and loss 0.5093007683753967\n",
      "epochs 10/400 and loss 0.5092991590499878\n",
      "epochs 11/400 and loss 0.5092957615852356\n",
      "epochs 12/400 and loss 0.509297251701355\n",
      "epochs 13/400 and loss 0.5092989206314087\n",
      "epochs 14/400 and loss 0.5092970132827759\n",
      "epochs 15/400 and loss 0.5092998743057251\n",
      "epochs 16/400 and loss 0.5092996954917908\n",
      "epochs 17/400 and loss 0.5092983245849609\n",
      "epochs 18/400 and loss 0.5092986822128296\n",
      "epochs 19/400 and loss 0.5092975497245789\n",
      "epochs 20/400 and loss 0.5092942118644714\n",
      "epochs 21/400 and loss 0.509297251701355\n",
      "epochs 22/400 and loss 0.5092962980270386\n",
      "epochs 23/400 and loss 0.5093001127243042\n",
      "epochs 24/400 and loss 0.50929856300354\n",
      "epochs 25/400 and loss 0.5092951655387878\n",
      "epochs 26/400 and loss 0.5092970132827759\n",
      "epochs 27/400 and loss 0.5092991590499878\n",
      "epochs 28/400 and loss 0.509299635887146\n",
      "epochs 29/400 and loss 0.5092962980270386\n",
      "epochs 30/400 and loss 0.5092951059341431\n",
      "epochs 31/400 and loss 0.5092969536781311\n",
      "epochs 32/400 and loss 0.5092992186546326\n",
      "epochs 33/400 and loss 0.5092976093292236\n",
      "epochs 34/400 and loss 0.5092947483062744\n",
      "epochs 35/400 and loss 0.5092971324920654\n",
      "epochs 36/400 and loss 0.5092968940734863\n",
      "epochs 37/400 and loss 0.5092993378639221\n",
      "epochs 38/400 and loss 0.5092977285385132\n",
      "epochs 39/400 and loss 0.5092936754226685\n",
      "epochs 40/400 and loss 0.5092953443527222\n",
      "epochs 41/400 and loss 0.5092967748641968\n",
      "epochs 42/400 and loss 0.509296178817749\n",
      "epochs 43/400 and loss 0.5092936754226685\n",
      "epochs 44/400 and loss 0.509297251701355\n",
      "epochs 45/400 and loss 0.5092951059341431\n",
      "epochs 46/400 and loss 0.5092939734458923\n",
      "epochs 47/400 and loss 0.5092951059341431\n",
      "epochs 48/400 and loss 0.5092986822128296\n",
      "epochs 49/400 and loss 0.5092961192131042\n",
      "epochs 50/400 and loss 0.5092920660972595\n",
      "epochs 51/400 and loss 0.5092958211898804\n",
      "epochs 52/400 and loss 0.5092938542366028\n",
      "epochs 53/400 and loss 0.5092955231666565\n",
      "epochs 54/400 and loss 0.5092957019805908\n",
      "epochs 55/400 and loss 0.5092951059341431\n",
      "epochs 56/400 and loss 0.5092949271202087\n",
      "epochs 57/400 and loss 0.5092952251434326\n",
      "epochs 58/400 and loss 0.5092941522598267\n",
      "epochs 59/400 and loss 0.5092965364456177\n",
      "epochs 60/400 and loss 0.5092947483062744\n",
      "epochs 61/400 and loss 0.5092942118644714\n",
      "epochs 62/400 and loss 0.5092976689338684\n",
      "epochs 63/400 and loss 0.5092974901199341\n",
      "epochs 64/400 and loss 0.5092960000038147\n",
      "epochs 65/400 and loss 0.5092949271202087\n",
      "epochs 66/400 and loss 0.5092941522598267\n",
      "epochs 67/400 and loss 0.5092946290969849\n",
      "epochs 68/400 and loss 0.5092942714691162\n",
      "epochs 69/400 and loss 0.5092912912368774\n",
      "epochs 70/400 and loss 0.509292721748352\n",
      "epochs 71/400 and loss 0.5092917680740356\n",
      "epochs 72/400 and loss 0.5092961192131042\n",
      "epochs 73/400 and loss 0.5092941522598267\n",
      "epochs 74/400 and loss 0.5092926025390625\n",
      "epochs 75/400 and loss 0.5092915892601013\n",
      "epochs 76/400 and loss 0.5092931985855103\n",
      "epochs 77/400 and loss 0.5092952251434326\n",
      "epochs 78/400 and loss 0.5092943906784058\n",
      "epochs 79/400 and loss 0.5092945098876953\n",
      "epochs 80/400 and loss 0.5092917680740356\n",
      "epochs 81/400 and loss 0.5092934370040894\n",
      "epochs 82/400 and loss 0.5092942118644714\n",
      "epochs 83/400 and loss 0.5092957615852356\n",
      "epochs 84/400 and loss 0.5092942118644714\n",
      "epochs 85/400 and loss 0.5092934966087341\n",
      "epochs 86/400 and loss 0.509292721748352\n",
      "epochs 87/400 and loss 0.50929194688797\n",
      "epochs 88/400 and loss 0.5092924237251282\n",
      "epochs 89/400 and loss 0.5092939138412476\n",
      "epochs 90/400 and loss 0.5092936754226685\n",
      "epochs 91/400 and loss 0.5092924237251282\n",
      "epochs 92/400 and loss 0.5092912912368774\n",
      "epochs 93/400 and loss 0.5092938542366028\n",
      "epochs 94/400 and loss 0.5092931985855103\n",
      "epochs 95/400 and loss 0.5092910528182983\n",
      "epochs 96/400 and loss 0.5092921257019043\n",
      "epochs 97/400 and loss 0.5092930197715759\n",
      "epochs 98/400 and loss 0.509292721748352\n",
      "epochs 99/400 and loss 0.5092936158180237\n",
      "epochs 100/400 and loss 0.509289562702179\n",
      "epochs 101/400 and loss 0.5092926621437073\n",
      "epochs 102/400 and loss 0.5092898011207581\n",
      "epochs 103/400 and loss 0.5092936754226685\n",
      "epochs 104/400 and loss 0.5092914700508118\n",
      "epochs 105/400 and loss 0.5092922449111938\n",
      "epochs 106/400 and loss 0.5092909932136536\n",
      "epochs 107/400 and loss 0.5092920064926147\n",
      "epochs 108/400 and loss 0.5092905759811401\n",
      "epochs 109/400 and loss 0.5092903971672058\n",
      "epochs 110/400 and loss 0.5092933773994446\n",
      "epochs 111/400 and loss 0.5092915296554565\n",
      "epochs 112/400 and loss 0.5092944502830505\n",
      "epochs 113/400 and loss 0.5092893242835999\n",
      "epochs 114/400 and loss 0.5092889666557312\n",
      "epochs 115/400 and loss 0.5092916488647461\n",
      "epochs 116/400 and loss 0.5092906951904297\n",
      "epochs 117/400 and loss 0.5092914700508118\n",
      "epochs 118/400 and loss 0.5092906951904297\n",
      "epochs 119/400 and loss 0.5092917680740356\n",
      "epochs 120/400 and loss 0.5092922449111938\n",
      "epochs 121/400 and loss 0.5092926025390625\n",
      "epochs 122/400 and loss 0.5092906951904297\n",
      "epochs 123/400 and loss 0.509290337562561\n",
      "epochs 124/400 and loss 0.5092938542366028\n",
      "epochs 125/400 and loss 0.5092934370040894\n",
      "epochs 126/400 and loss 0.5092903971672058\n",
      "epochs 127/400 and loss 0.5092899203300476\n",
      "epochs 128/400 and loss 0.5092923045158386\n",
      "epochs 129/400 and loss 0.5092884302139282\n",
      "epochs 130/400 and loss 0.5092886686325073\n",
      "epochs 131/400 and loss 0.5092911124229431\n",
      "epochs 132/400 and loss 0.5092899203300476\n",
      "epochs 133/400 and loss 0.5092912912368774\n",
      "epochs 134/400 and loss 0.5092898011207581\n",
      "epochs 135/400 and loss 0.5092910528182983\n",
      "epochs 136/400 and loss 0.5092892050743103\n",
      "epochs 137/400 and loss 0.5092896819114685\n",
      "epochs 138/400 and loss 0.5092926025390625\n",
      "epochs 139/400 and loss 0.5092895030975342\n",
      "epochs 140/400 and loss 0.509291410446167\n",
      "epochs 141/400 and loss 0.5092891454696655\n",
      "epochs 142/400 and loss 0.5092891454696655\n",
      "epochs 143/400 and loss 0.5092886090278625\n",
      "epochs 144/400 and loss 0.5092902183532715\n",
      "epochs 145/400 and loss 0.5092900991439819\n",
      "epochs 146/400 and loss 0.5092898607254028\n",
      "epochs 147/400 and loss 0.50928795337677\n",
      "epochs 148/400 and loss 0.5092877149581909\n",
      "epochs 149/400 and loss 0.5092906355857849\n",
      "epochs 150/400 and loss 0.5092881917953491\n",
      "epochs 151/400 and loss 0.5092875361442566\n",
      "epochs 152/400 and loss 0.5092907547950745\n",
      "epochs 153/400 and loss 0.5092899799346924\n",
      "epochs 154/400 and loss 0.509288489818573\n",
      "epochs 155/400 and loss 0.5092892646789551\n",
      "epochs 156/400 and loss 0.5092922449111938\n",
      "epochs 157/400 and loss 0.5092875361442566\n",
      "epochs 158/400 and loss 0.5092867016792297\n",
      "epochs 159/400 and loss 0.5092858672142029\n",
      "epochs 160/400 and loss 0.5092868804931641\n",
      "epochs 161/400 and loss 0.5092886090278625\n",
      "epochs 162/400 and loss 0.5092875361442566\n",
      "epochs 163/400 and loss 0.5092917084693909\n",
      "epochs 164/400 and loss 0.5092872977256775\n",
      "epochs 165/400 and loss 0.5092892050743103\n",
      "epochs 166/400 and loss 0.5092903971672058\n",
      "epochs 167/400 and loss 0.509289026260376\n",
      "epochs 168/400 and loss 0.509289026260376\n",
      "epochs 169/400 and loss 0.5092868804931641\n",
      "epochs 170/400 and loss 0.5092877745628357\n",
      "epochs 171/400 and loss 0.5092875957489014\n",
      "epochs 172/400 and loss 0.509290337562561\n",
      "epochs 173/400 and loss 0.5092892050743103\n",
      "epochs 174/400 and loss 0.509289026260376\n",
      "epochs 175/400 and loss 0.5092860460281372\n",
      "epochs 176/400 and loss 0.5092889070510864\n",
      "epochs 177/400 and loss 0.5092869400978088\n",
      "epochs 178/400 and loss 0.5092881917953491\n",
      "epochs 179/400 and loss 0.5092870593070984\n",
      "epochs 180/400 and loss 0.5092898011207581\n",
      "epochs 181/400 and loss 0.5092884302139282\n",
      "epochs 182/400 and loss 0.5092868804931641\n",
      "epochs 183/400 and loss 0.5092871189117432\n",
      "epochs 184/400 and loss 0.5092872977256775\n",
      "epochs 185/400 and loss 0.5092869400978088\n",
      "epochs 186/400 and loss 0.5092860460281372\n",
      "epochs 187/400 and loss 0.50928795337677\n",
      "epochs 188/400 and loss 0.5092869997024536\n",
      "epochs 189/400 and loss 0.5092883706092834\n",
      "epochs 190/400 and loss 0.5092834234237671\n",
      "epochs 191/400 and loss 0.5092872977256775\n",
      "epochs 192/400 and loss 0.5092853903770447\n",
      "epochs 193/400 and loss 0.5092865824699402\n",
      "epochs 194/400 and loss 0.509288489818573\n",
      "epochs 195/400 and loss 0.5092881917953491\n",
      "epochs 196/400 and loss 0.5092878937721252\n",
      "epochs 197/400 and loss 0.5092855095863342\n",
      "epochs 198/400 and loss 0.5092872381210327\n",
      "epochs 199/400 and loss 0.5092862248420715\n",
      "epochs 200/400 and loss 0.5092856884002686\n",
      "epochs 201/400 and loss 0.5092906355857849\n",
      "epochs 202/400 and loss 0.5092853307723999\n",
      "epochs 203/400 and loss 0.5092847347259521\n",
      "epochs 204/400 and loss 0.5092880129814148\n",
      "epochs 205/400 and loss 0.5092880725860596\n",
      "epochs 206/400 and loss 0.5092843770980835\n",
      "epochs 207/400 and loss 0.5092836618423462\n",
      "epochs 208/400 and loss 0.5092889070510864\n",
      "epochs 209/400 and loss 0.5092864632606506\n",
      "epochs 210/400 and loss 0.5092865228652954\n",
      "epochs 211/400 and loss 0.5092844367027283\n",
      "epochs 212/400 and loss 0.5092846155166626\n",
      "epochs 213/400 and loss 0.5092859268188477\n",
      "epochs 214/400 and loss 0.5092865228652954\n",
      "epochs 215/400 and loss 0.5092863440513611\n",
      "epochs 216/400 and loss 0.5092867612838745\n",
      "epochs 217/400 and loss 0.5092846155166626\n",
      "epochs 218/400 and loss 0.5092850923538208\n",
      "epochs 219/400 and loss 0.5092872381210327\n",
      "epochs 220/400 and loss 0.5092865228652954\n",
      "epochs 221/400 and loss 0.5092844367027283\n",
      "epochs 222/400 and loss 0.5092855095863342\n",
      "epochs 223/400 and loss 0.5092858076095581\n",
      "epochs 224/400 and loss 0.5092852711677551\n",
      "epochs 225/400 and loss 0.5092872381210327\n",
      "epochs 226/400 and loss 0.509284496307373\n",
      "epochs 227/400 and loss 0.5092846155166626\n",
      "epochs 228/400 and loss 0.509283185005188\n",
      "epochs 229/400 and loss 0.5092852711677551\n",
      "epochs 230/400 and loss 0.5092861652374268\n",
      "epochs 231/400 and loss 0.5092853307723999\n",
      "epochs 232/400 and loss 0.5092834234237671\n",
      "epochs 233/400 and loss 0.5092865228652954\n",
      "epochs 234/400 and loss 0.509284496307373\n",
      "epochs 235/400 and loss 0.5092860460281372\n",
      "epochs 236/400 and loss 0.5092836618423462\n",
      "epochs 237/400 and loss 0.509283185005188\n",
      "epochs 238/400 and loss 0.5092853903770447\n",
      "epochs 239/400 and loss 0.5092838406562805\n",
      "epochs 240/400 and loss 0.5092872381210327\n",
      "epochs 241/400 and loss 0.5092847943305969\n",
      "epochs 242/400 and loss 0.5092862844467163\n",
      "epochs 243/400 and loss 0.5092815160751343\n",
      "epochs 244/400 and loss 0.5092837810516357\n",
      "epochs 245/400 and loss 0.5092829465866089\n",
      "epochs 246/400 and loss 0.509286642074585\n",
      "epochs 247/400 and loss 0.5092836022377014\n",
      "epochs 248/400 and loss 0.5092843174934387\n",
      "epochs 249/400 and loss 0.509284257888794\n",
      "epochs 250/400 and loss 0.5092839002609253\n",
      "epochs 251/400 and loss 0.5092853307723999\n",
      "epochs 252/400 and loss 0.5092818737030029\n",
      "epochs 253/400 and loss 0.5092836022377014\n",
      "epochs 254/400 and loss 0.5092845559120178\n",
      "epochs 255/400 and loss 0.5092839002609253\n",
      "epochs 256/400 and loss 0.5092850923538208\n",
      "epochs 257/400 and loss 0.5092847943305969\n",
      "epochs 258/400 and loss 0.5092846155166626\n",
      "epochs 259/400 and loss 0.5092815160751343\n",
      "epochs 260/400 and loss 0.5092850923538208\n",
      "epochs 261/400 and loss 0.5092824101448059\n",
      "epochs 262/400 and loss 0.5092849135398865\n",
      "epochs 263/400 and loss 0.5092834234237671\n",
      "epochs 264/400 and loss 0.5092829465866089\n",
      "epochs 265/400 and loss 0.5092860460281372\n",
      "epochs 266/400 and loss 0.5092827081680298\n",
      "epochs 267/400 and loss 0.5092834234237671\n",
      "epochs 268/400 and loss 0.509281575679779\n",
      "epochs 269/400 and loss 0.5092827081680298\n",
      "epochs 270/400 and loss 0.5092844367027283\n",
      "epochs 271/400 and loss 0.509282648563385\n",
      "epochs 272/400 and loss 0.5092796087265015\n",
      "epochs 273/400 and loss 0.5092848539352417\n",
      "epochs 274/400 and loss 0.5092847347259521\n",
      "epochs 275/400 and loss 0.5092815160751343\n",
      "epochs 276/400 and loss 0.5092843770980835\n",
      "epochs 277/400 and loss 0.5092830657958984\n",
      "epochs 278/400 and loss 0.5092833042144775\n",
      "epochs 279/400 and loss 0.5092843770980835\n",
      "epochs 280/400 and loss 0.5092798471450806\n",
      "epochs 281/400 and loss 0.509283185005188\n",
      "epochs 282/400 and loss 0.5092827081680298\n",
      "epochs 283/400 and loss 0.5092813372612\n",
      "epochs 284/400 and loss 0.5092818140983582\n",
      "epochs 285/400 and loss 0.5092857480049133\n",
      "epochs 286/400 and loss 0.5092846155166626\n",
      "epochs 287/400 and loss 0.5092827677726746\n",
      "epochs 288/400 and loss 0.509280800819397\n",
      "epochs 289/400 and loss 0.5092832446098328\n",
      "epochs 290/400 and loss 0.5092839002609253\n",
      "epochs 291/400 and loss 0.5092816948890686\n",
      "epochs 292/400 and loss 0.5092848539352417\n",
      "epochs 293/400 and loss 0.5092812776565552\n",
      "epochs 294/400 and loss 0.5092864036560059\n",
      "epochs 295/400 and loss 0.5092810392379761\n",
      "epochs 296/400 and loss 0.509280800819397\n",
      "epochs 297/400 and loss 0.5092834234237671\n",
      "epochs 298/400 and loss 0.5092819929122925\n",
      "epochs 299/400 and loss 0.5092836618423462\n",
      "epochs 300/400 and loss 0.509280800819397\n",
      "epochs 301/400 and loss 0.5092805624008179\n",
      "epochs 302/400 and loss 0.509282112121582\n",
      "epochs 303/400 and loss 0.509280800819397\n",
      "epochs 304/400 and loss 0.5092790126800537\n",
      "epochs 305/400 and loss 0.5092835426330566\n",
      "epochs 306/400 and loss 0.5092812776565552\n",
      "epochs 307/400 and loss 0.5092798471450806\n",
      "epochs 308/400 and loss 0.5092820525169373\n",
      "epochs 309/400 and loss 0.5092804431915283\n",
      "epochs 310/400 and loss 0.5092836618423462\n",
      "epochs 311/400 and loss 0.5092822909355164\n",
      "epochs 312/400 and loss 0.509279191493988\n",
      "epochs 313/400 and loss 0.5092822909355164\n",
      "epochs 314/400 and loss 0.5092808604240417\n",
      "epochs 315/400 and loss 0.5092844367027283\n",
      "epochs 316/400 and loss 0.5092803239822388\n",
      "epochs 317/400 and loss 0.5092824697494507\n",
      "epochs 318/400 and loss 0.5092822313308716\n",
      "epochs 319/400 and loss 0.5092806220054626\n",
      "epochs 320/400 and loss 0.5092780590057373\n",
      "epochs 321/400 and loss 0.5092818140983582\n",
      "epochs 322/400 and loss 0.5092827081680298\n",
      "epochs 323/400 and loss 0.509279727935791\n",
      "epochs 324/400 and loss 0.5092819333076477\n",
      "epochs 325/400 and loss 0.5092805027961731\n",
      "epochs 326/400 and loss 0.5092843174934387\n",
      "epochs 327/400 and loss 0.5092813372612\n",
      "epochs 328/400 and loss 0.5092787146568298\n",
      "epochs 329/400 and loss 0.5092805624008179\n",
      "epochs 330/400 and loss 0.5092824101448059\n",
      "epochs 331/400 and loss 0.5092825293540955\n",
      "epochs 332/400 and loss 0.5092817544937134\n",
      "epochs 333/400 and loss 0.5092796087265015\n",
      "epochs 334/400 and loss 0.5092833638191223\n",
      "epochs 335/400 and loss 0.5092805027961731\n",
      "epochs 336/400 and loss 0.5092779397964478\n",
      "epochs 337/400 and loss 0.5092824697494507\n",
      "epochs 338/400 and loss 0.5092817544937134\n",
      "epochs 339/400 and loss 0.5092760920524597\n",
      "epochs 340/400 and loss 0.5092772245407104\n",
      "epochs 341/400 and loss 0.5092792510986328\n",
      "epochs 342/400 and loss 0.5092813372612\n",
      "epochs 343/400 and loss 0.5092796683311462\n",
      "epochs 344/400 and loss 0.5092798471450806\n",
      "epochs 345/400 and loss 0.5092818737030029\n",
      "epochs 346/400 and loss 0.5092790126800537\n",
      "epochs 347/400 and loss 0.5092805624008179\n",
      "epochs 348/400 and loss 0.5092798471450806\n",
      "epochs 349/400 and loss 0.5092786550521851\n",
      "epochs 350/400 and loss 0.5092819333076477\n",
      "epochs 351/400 and loss 0.5092805624008179\n",
      "epochs 352/400 and loss 0.5092761516571045\n",
      "epochs 353/400 and loss 0.5092808604240417\n",
      "epochs 354/400 and loss 0.5092788934707642\n",
      "epochs 355/400 and loss 0.5092775821685791\n",
      "epochs 356/400 and loss 0.5092789530754089\n",
      "epochs 357/400 and loss 0.5092792510986328\n",
      "epochs 358/400 and loss 0.509282112121582\n",
      "epochs 359/400 and loss 0.5092789530754089\n",
      "epochs 360/400 and loss 0.5092780590057373\n",
      "epochs 361/400 and loss 0.5092800259590149\n",
      "epochs 362/400 and loss 0.5092798471450806\n",
      "epochs 363/400 and loss 0.5092819929122925\n",
      "epochs 364/400 and loss 0.5092797875404358\n",
      "epochs 365/400 and loss 0.509278416633606\n",
      "epochs 366/400 and loss 0.5092816948890686\n",
      "epochs 367/400 and loss 0.5092795491218567\n",
      "epochs 368/400 and loss 0.5092768669128418\n",
      "epochs 369/400 and loss 0.5092796087265015\n",
      "epochs 370/400 and loss 0.509280800819397\n",
      "epochs 371/400 and loss 0.5092775821685791\n",
      "epochs 372/400 and loss 0.5092784762382507\n",
      "epochs 373/400 and loss 0.5092790722846985\n",
      "epochs 374/400 and loss 0.5092808604240417\n",
      "epochs 375/400 and loss 0.509278416633606\n",
      "epochs 376/400 and loss 0.5092781782150269\n",
      "epochs 377/400 and loss 0.5092803239822388\n",
      "epochs 378/400 and loss 0.5092779397964478\n",
      "epochs 379/400 and loss 0.5092812180519104\n",
      "epochs 380/400 and loss 0.5092782378196716\n",
      "epochs 381/400 and loss 0.5092772841453552\n",
      "epochs 382/400 and loss 0.5092813968658447\n",
      "epochs 383/400 and loss 0.5092805027961731\n",
      "epochs 384/400 and loss 0.5092781782150269\n",
      "epochs 385/400 and loss 0.5092782378196716\n",
      "epochs 386/400 and loss 0.5092777609825134\n",
      "epochs 387/400 and loss 0.5092777013778687\n",
      "epochs 388/400 and loss 0.50927734375\n",
      "epochs 389/400 and loss 0.5092751979827881\n",
      "epochs 390/400 and loss 0.5092808604240417\n",
      "epochs 391/400 and loss 0.5092796683311462\n",
      "epochs 392/400 and loss 0.5092741250991821\n",
      "epochs 393/400 and loss 0.5092805624008179\n",
      "epochs 394/400 and loss 0.5092789530754089\n",
      "epochs 395/400 and loss 0.5092783570289612\n",
      "epochs 396/400 and loss 0.5092774629592896\n",
      "epochs 397/400 and loss 0.5092777013778687\n",
      "epochs 398/400 and loss 0.5092811584472656\n",
      "epochs 399/400 and loss 0.5092786550521851\n",
      "epochs 400/400 and loss 0.5092756152153015\n",
      "epochs 401/400 and loss 0.5092760920524597\n",
      "epochs 402/400 and loss 0.5092778205871582\n",
      "epochs 403/400 and loss 0.509276807308197\n",
      "epochs 404/400 and loss 0.5092794299125671\n",
      "epochs 405/400 and loss 0.5092779397964478\n",
      "epochs 406/400 and loss 0.5092803239822388\n",
      "epochs 407/400 and loss 0.5092769265174866\n",
      "epochs 408/400 and loss 0.5092766880989075\n",
      "epochs 409/400 and loss 0.5092796087265015\n",
      "epochs 410/400 and loss 0.5092790722846985\n",
      "epochs 411/400 and loss 0.5092803239822388\n",
      "epochs 412/400 and loss 0.5092765092849731\n",
      "epochs 413/400 and loss 0.5092765688896179\n",
      "epochs 414/400 and loss 0.5092786550521851\n",
      "epochs 415/400 and loss 0.5092772245407104\n",
      "epochs 416/400 and loss 0.5092786550521851\n",
      "epochs 417/400 and loss 0.5092789530754089\n",
      "epochs 418/400 and loss 0.5092776417732239\n",
      "epochs 419/400 and loss 0.5092765092849731\n",
      "epochs 420/400 and loss 0.5092781782150269\n",
      "epochs 421/400 and loss 0.5092750787734985\n",
      "epochs 422/400 and loss 0.5092788338661194\n",
      "epochs 423/400 and loss 0.50927734375\n",
      "epochs 424/400 and loss 0.509275496006012\n",
      "epochs 425/400 and loss 0.5092771053314209\n",
      "epochs 426/400 and loss 0.5092750787734985\n",
      "epochs 427/400 and loss 0.5092754364013672\n",
      "epochs 428/400 and loss 0.5092752575874329\n",
      "epochs 429/400 and loss 0.5092756152153015\n",
      "epochs 430/400 and loss 0.5092791318893433\n",
      "epochs 431/400 and loss 0.5092769861221313\n",
      "epochs 432/400 and loss 0.5092734098434448\n",
      "epochs 433/400 and loss 0.5092777609825134\n",
      "epochs 434/400 and loss 0.5092764496803284\n",
      "epochs 435/400 and loss 0.5092779994010925\n",
      "epochs 436/400 and loss 0.5092756152153015\n",
      "epochs 437/400 and loss 0.5092758536338806\n",
      "epochs 438/400 and loss 0.5092805027961731\n",
      "epochs 439/400 and loss 0.5092743635177612\n",
      "epochs 440/400 and loss 0.5092762112617493\n",
      "epochs 441/400 and loss 0.5092760324478149\n",
      "epochs 442/400 and loss 0.5092775821685791\n",
      "epochs 443/400 and loss 0.5092774033546448\n",
      "epochs 444/400 and loss 0.5092760920524597\n",
      "epochs 445/400 and loss 0.5092761516571045\n",
      "epochs 446/400 and loss 0.509278416633606\n",
      "epochs 447/400 and loss 0.5092768669128418\n",
      "epochs 448/400 and loss 0.509273886680603\n",
      "epochs 449/400 and loss 0.5092791318893433\n",
      "epochs 450/400 and loss 0.5092767477035522\n",
      "epochs 451/400 and loss 0.5092743039131165\n",
      "epochs 452/400 and loss 0.5092750787734985\n",
      "epochs 453/400 and loss 0.5092761516571045\n",
      "epochs 454/400 and loss 0.5092780590057373\n",
      "epochs 455/400 and loss 0.5092757940292358\n",
      "epochs 456/400 and loss 0.5092750191688538\n",
      "epochs 457/400 and loss 0.5092759728431702\n",
      "epochs 458/400 and loss 0.5092781782150269\n",
      "epochs 459/400 and loss 0.5092775821685791\n",
      "epochs 460/400 and loss 0.5092763900756836\n",
      "epochs 461/400 and loss 0.5092740654945374\n",
      "epochs 462/400 and loss 0.5092760324478149\n",
      "epochs 463/400 and loss 0.509275496006012\n",
      "epochs 464/400 and loss 0.5092747211456299\n",
      "epochs 465/400 and loss 0.5092750191688538\n",
      "epochs 466/400 and loss 0.5092774033546448\n",
      "epochs 467/400 and loss 0.5092750191688538\n",
      "epochs 468/400 and loss 0.5092742443084717\n",
      "epochs 469/400 and loss 0.5092746019363403\n",
      "epochs 470/400 and loss 0.5092769861221313\n",
      "epochs 471/400 and loss 0.509273886680603\n",
      "epochs 472/400 and loss 0.5092731714248657\n",
      "epochs 473/400 and loss 0.5092746019363403\n",
      "epochs 474/400 and loss 0.5092746615409851\n",
      "epochs 475/400 and loss 0.509276270866394\n",
      "epochs 476/400 and loss 0.5092741250991821\n",
      "epochs 477/400 and loss 0.5092765092849731\n",
      "epochs 478/400 and loss 0.5092774033546448\n",
      "epochs 479/400 and loss 0.5092760324478149\n",
      "epochs 480/400 and loss 0.5092740058898926\n",
      "epochs 481/400 and loss 0.5092753171920776\n",
      "epochs 482/400 and loss 0.5092777609825134\n",
      "epochs 483/400 and loss 0.5092719793319702\n",
      "epochs 484/400 and loss 0.5092782378196716\n",
      "epochs 485/400 and loss 0.5092746615409851\n",
      "epochs 486/400 and loss 0.5092767477035522\n",
      "epochs 487/400 and loss 0.5092746019363403\n",
      "epochs 488/400 and loss 0.509273111820221\n",
      "epochs 489/400 and loss 0.5092748999595642\n",
      "epochs 490/400 and loss 0.5092751383781433\n",
      "epochs 491/400 and loss 0.5092757940292358\n",
      "epochs 492/400 and loss 0.5092741250991821\n",
      "epochs 493/400 and loss 0.5092758536338806\n",
      "epochs 494/400 and loss 0.5092775225639343\n",
      "epochs 495/400 and loss 0.5092726349830627\n",
      "epochs 496/400 and loss 0.5092739462852478\n",
      "epochs 497/400 and loss 0.5092725157737732\n",
      "epochs 498/400 and loss 0.5092753767967224\n",
      "epochs 499/400 and loss 0.5092743635177612\n",
      "epochs 500/400 and loss 0.5092736482620239\n",
      "epochs 501/400 and loss 0.5092744827270508\n",
      "epochs 502/400 and loss 0.5092746019363403\n",
      "epochs 503/400 and loss 0.5092755556106567\n",
      "epochs 504/400 and loss 0.509273886680603\n",
      "epochs 505/400 and loss 0.5092753767967224\n",
      "epochs 506/400 and loss 0.5092739462852478\n",
      "epochs 507/400 and loss 0.5092723965644836\n",
      "epochs 508/400 and loss 0.5092751979827881\n",
      "epochs 509/400 and loss 0.5092733502388\n",
      "epochs 510/400 and loss 0.5092765092849731\n",
      "epochs 511/400 and loss 0.5092737078666687\n",
      "epochs 512/400 and loss 0.5092746019363403\n",
      "epochs 513/400 and loss 0.509276270866394\n",
      "epochs 514/400 and loss 0.5092722177505493\n",
      "epochs 515/400 and loss 0.509273886680603\n",
      "epochs 516/400 and loss 0.5092746019363403\n",
      "epochs 517/400 and loss 0.5092751979827881\n",
      "epochs 518/400 and loss 0.509271502494812\n",
      "epochs 519/400 and loss 0.5092753171920776\n",
      "epochs 520/400 and loss 0.5092707872390747\n",
      "epochs 521/400 and loss 0.5092741250991821\n",
      "epochs 522/400 and loss 0.5092747211456299\n",
      "epochs 523/400 and loss 0.509272575378418\n",
      "epochs 524/400 and loss 0.509270966053009\n",
      "epochs 525/400 and loss 0.5092737674713135\n",
      "epochs 526/400 and loss 0.5092760324478149\n",
      "epochs 527/400 and loss 0.5092731714248657\n",
      "epochs 528/400 and loss 0.5092757344245911\n",
      "epochs 529/400 and loss 0.5092738270759583\n",
      "epochs 530/400 and loss 0.5092741847038269\n",
      "epochs 531/400 and loss 0.5092753171920776\n",
      "epochs 532/400 and loss 0.5092751383781433\n",
      "epochs 533/400 and loss 0.5092744827270508\n",
      "epochs 534/400 and loss 0.5092723965644836\n",
      "epochs 535/400 and loss 0.5092740654945374\n",
      "epochs 536/400 and loss 0.5092740058898926\n",
      "epochs 537/400 and loss 0.5092713832855225\n",
      "epochs 538/400 and loss 0.509275496006012\n",
      "epochs 539/400 and loss 0.5092708468437195\n",
      "epochs 540/400 and loss 0.509273886680603\n",
      "epochs 541/400 and loss 0.5092734098434448\n",
      "epochs 542/400 and loss 0.5092725157737732\n",
      "epochs 543/400 and loss 0.5092725157737732\n",
      "epochs 544/400 and loss 0.5092733502388\n",
      "epochs 545/400 and loss 0.5092730522155762\n",
      "epochs 546/400 and loss 0.5092726349830627\n",
      "epochs 547/400 and loss 0.5092731714248657\n",
      "epochs 548/400 and loss 0.509271502494812\n",
      "epochs 549/400 and loss 0.5092723965644836\n",
      "epochs 550/400 and loss 0.5092725157737732\n",
      "epochs 551/400 and loss 0.5092716813087463\n",
      "epochs 552/400 and loss 0.5092731714248657\n",
      "epochs 553/400 and loss 0.5092743635177612\n",
      "epochs 554/400 and loss 0.5092705488204956\n",
      "epochs 555/400 and loss 0.5092767477035522\n",
      "epochs 556/400 and loss 0.5092723965644836\n",
      "epochs 557/400 and loss 0.5092738270759583\n",
      "epochs 558/400 and loss 0.5092738270759583\n",
      "epochs 559/400 and loss 0.5092723369598389\n",
      "epochs 560/400 and loss 0.5092724561691284\n",
      "epochs 561/400 and loss 0.509273886680603\n",
      "epochs 562/400 and loss 0.5092710852622986\n",
      "epochs 563/400 and loss 0.5092743635177612\n",
      "epochs 564/400 and loss 0.5092716813087463\n",
      "epochs 565/400 and loss 0.5092731714248657\n",
      "epochs 566/400 and loss 0.5092704892158508\n",
      "epochs 567/400 and loss 0.5092712640762329\n",
      "epochs 568/400 and loss 0.5092743039131165\n",
      "epochs 569/400 and loss 0.5092744827270508\n",
      "epochs 570/400 and loss 0.5092739462852478\n",
      "epochs 571/400 and loss 0.5092722177505493\n",
      "epochs 572/400 and loss 0.5092729330062866\n",
      "epochs 573/400 and loss 0.5092734098434448\n",
      "epochs 574/400 and loss 0.5092728137969971\n",
      "epochs 575/400 and loss 0.5092718005180359\n",
      "epochs 576/400 and loss 0.5092741250991821\n",
      "epochs 577/400 and loss 0.5092700123786926\n",
      "epochs 578/400 and loss 0.5092706084251404\n",
      "epochs 579/400 and loss 0.509272038936615\n",
      "epochs 580/400 and loss 0.5092719197273254\n",
      "epochs 581/400 and loss 0.5092711448669434\n",
      "epochs 582/400 and loss 0.5092698931694031\n",
      "epochs 583/400 and loss 0.5092713832855225\n",
      "epochs 584/400 and loss 0.5092734694480896\n",
      "epochs 585/400 and loss 0.5092694759368896\n",
      "epochs 586/400 and loss 0.5092728734016418\n",
      "epochs 587/400 and loss 0.5092735886573792\n",
      "epochs 588/400 and loss 0.5092724561691284\n",
      "epochs 589/400 and loss 0.5092724561691284\n",
      "epochs 590/400 and loss 0.5092700719833374\n",
      "epochs 591/400 and loss 0.5092714428901672\n",
      "epochs 592/400 and loss 0.5092698335647583\n",
      "epochs 593/400 and loss 0.5092726945877075\n",
      "epochs 594/400 and loss 0.5092740654945374\n",
      "epochs 595/400 and loss 0.5092734098434448\n",
      "epochs 596/400 and loss 0.5092688798904419\n",
      "epochs 597/400 and loss 0.5092738270759583\n",
      "epochs 598/400 and loss 0.5092698931694031\n",
      "epochs 599/400 and loss 0.509272575378418\n",
      "epochs 600/400 and loss 0.509272575378418\n",
      "epochs 601/400 and loss 0.5092722773551941\n",
      "epochs 602/400 and loss 0.5092698931694031\n",
      "epochs 603/400 and loss 0.5092718005180359\n",
      "epochs 604/400 and loss 0.5092697739601135\n",
      "epochs 605/400 and loss 0.5092728734016418\n",
      "epochs 606/400 and loss 0.5092712640762329\n",
      "epochs 607/400 and loss 0.5092693567276001\n",
      "epochs 608/400 and loss 0.5092716217041016\n",
      "epochs 609/400 and loss 0.5092718601226807\n",
      "epochs 610/400 and loss 0.5092751979827881\n",
      "epochs 611/400 and loss 0.5092705488204956\n",
      "epochs 612/400 and loss 0.5092712640762329\n",
      "epochs 613/400 and loss 0.509270429611206\n",
      "epochs 614/400 and loss 0.5092719793319702\n",
      "epochs 615/400 and loss 0.5092732906341553\n",
      "epochs 616/400 and loss 0.5092714428901672\n",
      "epochs 617/400 and loss 0.5092707872390747\n",
      "epochs 618/400 and loss 0.5092703104019165\n",
      "epochs 619/400 and loss 0.509271502494812\n",
      "epochs 620/400 and loss 0.5092694759368896\n",
      "epochs 621/400 and loss 0.5092694163322449\n",
      "epochs 622/400 and loss 0.5092712044715881\n",
      "epochs 623/400 and loss 0.5092700719833374\n",
      "epochs 624/400 and loss 0.5092720985412598\n",
      "epochs 625/400 and loss 0.5092693567276001\n",
      "epochs 626/400 and loss 0.5092715620994568\n",
      "epochs 627/400 and loss 0.5092698931694031\n",
      "epochs 628/400 and loss 0.5092691779136658\n",
      "epochs 629/400 and loss 0.5092731714248657\n",
      "epochs 630/400 and loss 0.5092721581459045\n",
      "epochs 631/400 and loss 0.5092712640762329\n",
      "epochs 632/400 and loss 0.5092712044715881\n",
      "epochs 633/400 and loss 0.5092705488204956\n",
      "epochs 634/400 and loss 0.5092703700065613\n",
      "epochs 635/400 and loss 0.5092710852622986\n",
      "epochs 636/400 and loss 0.5092707872390747\n",
      "epochs 637/400 and loss 0.5092722177505493\n",
      "epochs 638/400 and loss 0.5092692971229553\n",
      "epochs 639/400 and loss 0.5092719793319702\n",
      "epochs 640/400 and loss 0.5092707872390747\n",
      "epochs 641/400 and loss 0.5092687010765076\n",
      "epochs 642/400 and loss 0.5092722177505493\n",
      "epochs 643/400 and loss 0.5092707872390747\n",
      "epochs 644/400 and loss 0.509268581867218\n",
      "epochs 645/400 and loss 0.5092716217041016\n",
      "epochs 646/400 and loss 0.5092675685882568\n",
      "epochs 647/400 and loss 0.5092707276344299\n",
      "epochs 648/400 and loss 0.5092698335647583\n",
      "epochs 649/400 and loss 0.5092695951461792\n",
      "epochs 650/400 and loss 0.5092719197273254\n",
      "epochs 651/400 and loss 0.509268045425415\n",
      "epochs 652/400 and loss 0.5092719197273254\n",
      "epochs 653/400 and loss 0.5092707872390747\n",
      "epochs 654/400 and loss 0.5092682838439941\n",
      "epochs 655/400 and loss 0.5092703104019165\n",
      "epochs 656/400 and loss 0.5092710256576538\n",
      "epochs 657/400 and loss 0.5092707872390747\n",
      "epochs 658/400 and loss 0.5092698335647583\n",
      "epochs 659/400 and loss 0.509272575378418\n",
      "epochs 660/400 and loss 0.5092692971229553\n",
      "epochs 661/400 and loss 0.5092678070068359\n",
      "epochs 662/400 and loss 0.5092738270759583\n",
      "epochs 663/400 and loss 0.5092689394950867\n",
      "epochs 664/400 and loss 0.5092710256576538\n",
      "epochs 665/400 and loss 0.5092678070068359\n",
      "epochs 666/400 and loss 0.5092712640762329\n",
      "epochs 667/400 and loss 0.5092691779136658\n",
      "epochs 668/400 and loss 0.5092698335647583\n",
      "epochs 669/400 and loss 0.5092684030532837\n",
      "epochs 670/400 and loss 0.509269118309021\n",
      "epochs 671/400 and loss 0.5092712640762329\n",
      "epochs 672/400 and loss 0.509271502494812\n",
      "epochs 673/400 and loss 0.5092712640762329\n",
      "epochs 674/400 and loss 0.5092697739601135\n",
      "epochs 675/400 and loss 0.5092712640762329\n",
      "epochs 676/400 and loss 0.509270966053009\n",
      "epochs 677/400 and loss 0.5092712640762329\n",
      "epochs 678/400 and loss 0.509270191192627\n",
      "epochs 679/400 and loss 0.5092723965644836\n",
      "epochs 680/400 and loss 0.5092681646347046\n",
      "epochs 681/400 and loss 0.5092700123786926\n",
      "epochs 682/400 and loss 0.5092713832855225\n",
      "epochs 683/400 and loss 0.509270966053009\n",
      "epochs 684/400 and loss 0.5092673301696777\n",
      "epochs 685/400 and loss 0.5092705488204956\n",
      "epochs 686/400 and loss 0.5092676281929016\n",
      "epochs 687/400 and loss 0.5092699527740479\n",
      "epochs 688/400 and loss 0.5092690587043762\n",
      "epochs 689/400 and loss 0.5092666745185852\n",
      "epochs 690/400 and loss 0.5092700719833374\n",
      "epochs 691/400 and loss 0.509269118309021\n",
      "epochs 692/400 and loss 0.5092694759368896\n",
      "epochs 693/400 and loss 0.5092697739601135\n",
      "epochs 694/400 and loss 0.509269118309021\n",
      "epochs 695/400 and loss 0.5092689394950867\n",
      "epochs 696/400 and loss 0.5092674493789673\n",
      "epochs 697/400 and loss 0.5092686414718628\n",
      "epochs 698/400 and loss 0.5092710256576538\n",
      "epochs 699/400 and loss 0.5092653036117554\n",
      "epochs 700/400 and loss 0.509269118309021\n",
      "epochs 701/400 and loss 0.5092706084251404\n",
      "epochs 702/400 and loss 0.5092681050300598\n",
      "epochs 703/400 and loss 0.5092670321464539\n",
      "epochs 704/400 and loss 0.5092719793319702\n",
      "epochs 705/400 and loss 0.5092678666114807\n",
      "epochs 706/400 and loss 0.5092675089836121\n",
      "epochs 707/400 and loss 0.5092702507972717\n",
      "epochs 708/400 and loss 0.509269118309021\n",
      "epochs 709/400 and loss 0.509268045425415\n",
      "epochs 710/400 and loss 0.5092701315879822\n",
      "epochs 711/400 and loss 0.5092664361000061\n",
      "epochs 712/400 and loss 0.509269118309021\n",
      "epochs 713/400 and loss 0.5092672109603882\n",
      "epochs 714/400 and loss 0.509270191192627\n",
      "epochs 715/400 and loss 0.5092700123786926\n",
      "epochs 716/400 and loss 0.5092679262161255\n",
      "epochs 717/400 and loss 0.5092697143554688\n",
      "epochs 718/400 and loss 0.5092695951461792\n",
      "epochs 719/400 and loss 0.5092691779136658\n",
      "epochs 720/400 and loss 0.5092681646347046\n",
      "epochs 721/400 and loss 0.5092662572860718\n",
      "epochs 722/400 and loss 0.5092688798904419\n",
      "epochs 723/400 and loss 0.5092707872390747\n",
      "epochs 724/400 and loss 0.5092657208442688\n",
      "epochs 725/400 and loss 0.5092686414718628\n",
      "epochs 726/400 and loss 0.5092672109603882\n",
      "epochs 727/400 and loss 0.5092657804489136\n",
      "epochs 728/400 and loss 0.5092698335647583\n",
      "epochs 729/400 and loss 0.509268581867218\n",
      "epochs 730/400 and loss 0.5092698931694031\n",
      "epochs 731/400 and loss 0.509269654750824\n",
      "epochs 732/400 and loss 0.5092674493789673\n",
      "epochs 733/400 and loss 0.5092716813087463\n",
      "epochs 734/400 and loss 0.5092673897743225\n",
      "epochs 735/400 and loss 0.5092684626579285\n",
      "epochs 736/400 and loss 0.5092695951461792\n",
      "epochs 737/400 and loss 0.5092640519142151\n",
      "epochs 738/400 and loss 0.5092657804489136\n",
      "epochs 739/400 and loss 0.5092647671699524\n",
      "epochs 740/400 and loss 0.5092700719833374\n",
      "epochs 741/400 and loss 0.5092676281929016\n",
      "epochs 742/400 and loss 0.5092661380767822\n",
      "epochs 743/400 and loss 0.5092693567276001\n",
      "epochs 744/400 and loss 0.5092669725418091\n",
      "epochs 745/400 and loss 0.5092686414718628\n",
      "epochs 746/400 and loss 0.5092684030532837\n",
      "epochs 747/400 and loss 0.5092660784721375\n",
      "epochs 748/400 and loss 0.5092664957046509\n",
      "epochs 749/400 and loss 0.5092672109603882\n",
      "epochs 750/400 and loss 0.5092670321464539\n",
      "epochs 751/400 and loss 0.5092695951461792\n",
      "epochs 752/400 and loss 0.5092694163322449\n",
      "epochs 753/400 and loss 0.5092698335647583\n",
      "epochs 754/400 and loss 0.5092679858207703\n",
      "epochs 755/400 and loss 0.5092695355415344\n",
      "epochs 756/400 and loss 0.5092688202857971\n",
      "epochs 757/400 and loss 0.5092677474021912\n",
      "epochs 758/400 and loss 0.509268581867218\n",
      "epochs 759/400 and loss 0.5092681050300598\n",
      "epochs 760/400 and loss 0.5092663764953613\n",
      "epochs 761/400 and loss 0.5092668533325195\n",
      "epochs 762/400 and loss 0.5092675089836121\n",
      "epochs 763/400 and loss 0.5092657804489136\n",
      "epochs 764/400 and loss 0.50926673412323\n",
      "epochs 765/400 and loss 0.5092670917510986\n",
      "epochs 766/400 and loss 0.5092670917510986\n",
      "epochs 767/400 and loss 0.5092692375183105\n",
      "epochs 768/400 and loss 0.5092695951461792\n",
      "epochs 769/400 and loss 0.509265124797821\n",
      "epochs 770/400 and loss 0.5092683434486389\n",
      "epochs 771/400 and loss 0.5092654824256897\n",
      "epochs 772/400 and loss 0.5092700719833374\n",
      "epochs 773/400 and loss 0.5092676877975464\n",
      "epochs 774/400 and loss 0.5092676877975464\n",
      "epochs 775/400 and loss 0.509268581867218\n",
      "epochs 776/400 and loss 0.5092700719833374\n",
      "epochs 777/400 and loss 0.5092681050300598\n",
      "epochs 778/400 and loss 0.509268581867218\n",
      "epochs 779/400 and loss 0.5092663168907166\n",
      "epochs 780/400 and loss 0.5092672109603882\n",
      "epochs 781/400 and loss 0.5092671513557434\n",
      "epochs 782/400 and loss 0.5092664957046509\n",
      "epochs 783/400 and loss 0.5092663168907166\n",
      "epochs 784/400 and loss 0.509270429611206\n",
      "epochs 785/400 and loss 0.5092658400535583\n",
      "epochs 786/400 and loss 0.5092652440071106\n",
      "epochs 787/400 and loss 0.509268581867218\n",
      "epochs 788/400 and loss 0.5092665553092957\n",
      "epochs 789/400 and loss 0.5092681050300598\n",
      "epochs 790/400 and loss 0.5092726945877075\n",
      "epochs 791/400 and loss 0.5092651844024658\n",
      "epochs 792/400 and loss 0.5092676281929016\n",
      "epochs 793/400 and loss 0.5092664957046509\n",
      "epochs 794/400 and loss 0.5092671513557434\n",
      "epochs 795/400 and loss 0.5092678070068359\n",
      "epochs 796/400 and loss 0.5092639327049255\n",
      "epochs 797/400 and loss 0.5092685222625732\n",
      "epochs 798/400 and loss 0.5092639923095703\n",
      "epochs 799/400 and loss 0.5092657208442688\n",
      "epochs 800/400 and loss 0.5092674493789673\n",
      "epochs 801/400 and loss 0.5092666149139404\n",
      "epochs 802/400 and loss 0.5092685222625732\n",
      "epochs 803/400 and loss 0.50926673412323\n",
      "epochs 804/400 and loss 0.5092693567276001\n",
      "epochs 805/400 and loss 0.509265124797821\n",
      "epochs 806/400 and loss 0.5092666149139404\n",
      "epochs 807/400 and loss 0.5092672109603882\n",
      "epochs 808/400 and loss 0.5092666149139404\n",
      "epochs 809/400 and loss 0.5092672109603882\n",
      "epochs 810/400 and loss 0.5092675089836121\n",
      "epochs 811/400 and loss 0.5092638731002808\n",
      "epochs 812/400 and loss 0.5092681646347046\n",
      "epochs 813/400 and loss 0.5092689394950867\n",
      "epochs 814/400 and loss 0.5092679858207703\n",
      "epochs 815/400 and loss 0.5092633962631226\n",
      "epochs 816/400 and loss 0.5092672109603882\n",
      "epochs 817/400 and loss 0.509263813495636\n",
      "epochs 818/400 and loss 0.5092672109603882\n",
      "epochs 819/400 and loss 0.5092670321464539\n",
      "epochs 820/400 and loss 0.5092656016349792\n",
      "epochs 821/400 and loss 0.5092661380767822\n",
      "epochs 822/400 and loss 0.5092648863792419\n",
      "epochs 823/400 and loss 0.5092633962631226\n",
      "epochs 824/400 and loss 0.5092689990997314\n",
      "epochs 825/400 and loss 0.5092687010765076\n",
      "epochs 826/400 and loss 0.5092636346817017\n",
      "epochs 827/400 and loss 0.5092655420303345\n",
      "epochs 828/400 and loss 0.5092679858207703\n",
      "epochs 829/400 and loss 0.5092672109603882\n",
      "epochs 830/400 and loss 0.5092659592628479\n",
      "epochs 831/400 and loss 0.50926673412323\n",
      "epochs 832/400 and loss 0.5092678070068359\n",
      "epochs 833/400 and loss 0.5092664957046509\n",
      "epochs 834/400 and loss 0.5092689990997314\n",
      "epochs 835/400 and loss 0.5092668533325195\n",
      "epochs 836/400 and loss 0.5092672109603882\n",
      "epochs 837/400 and loss 0.5092651844024658\n",
      "epochs 838/400 and loss 0.5092666149139404\n",
      "epochs 839/400 and loss 0.5092669725418091\n",
      "epochs 840/400 and loss 0.5092669725418091\n",
      "epochs 841/400 and loss 0.5092666745185852\n",
      "epochs 842/400 and loss 0.5092664957046509\n",
      "epochs 843/400 and loss 0.5092645883560181\n",
      "epochs 844/400 and loss 0.5092662572860718\n",
      "epochs 845/400 and loss 0.5092686414718628\n",
      "epochs 846/400 and loss 0.5092647075653076\n",
      "epochs 847/400 and loss 0.5092645287513733\n",
      "epochs 848/400 and loss 0.5092681050300598\n",
      "epochs 849/400 and loss 0.5092663168907166\n",
      "epochs 850/400 and loss 0.5092684030532837\n",
      "epochs 851/400 and loss 0.5092653632164001\n",
      "epochs 852/400 and loss 0.5092675089836121\n",
      "epochs 853/400 and loss 0.5092650055885315\n",
      "epochs 854/400 and loss 0.5092666149139404\n",
      "epochs 855/400 and loss 0.5092653036117554\n",
      "epochs 856/400 and loss 0.5092660188674927\n",
      "epochs 857/400 and loss 0.5092652440071106\n",
      "epochs 858/400 and loss 0.5092672109603882\n",
      "epochs 859/400 and loss 0.5092629194259644\n",
      "epochs 860/400 and loss 0.5092679262161255\n",
      "epochs 861/400 and loss 0.5092665553092957\n",
      "epochs 862/400 and loss 0.5092647075653076\n",
      "epochs 863/400 and loss 0.5092649459838867\n",
      "epochs 864/400 and loss 0.509264349937439\n",
      "epochs 865/400 and loss 0.5092679858207703\n",
      "epochs 866/400 and loss 0.5092665553092957\n",
      "epochs 867/400 and loss 0.5092653632164001\n",
      "epochs 868/400 and loss 0.5092654228210449\n",
      "epochs 869/400 and loss 0.5092673301696777\n",
      "epochs 870/400 and loss 0.50926673412323\n",
      "epochs 871/400 and loss 0.5092649459838867\n",
      "epochs 872/400 and loss 0.5092698335647583\n",
      "epochs 873/400 and loss 0.5092653036117554\n",
      "epochs 874/400 and loss 0.5092647075653076\n",
      "epochs 875/400 and loss 0.5092660784721375\n",
      "epochs 876/400 and loss 0.5092653632164001\n",
      "epochs 877/400 and loss 0.5092649459838867\n",
      "epochs 878/400 and loss 0.5092648267745972\n",
      "epochs 879/400 and loss 0.5092636346817017\n",
      "epochs 880/400 and loss 0.509267270565033\n",
      "epochs 881/400 and loss 0.5092660784721375\n",
      "epochs 882/400 and loss 0.5092687010765076\n",
      "epochs 883/400 and loss 0.5092657208442688\n",
      "epochs 884/400 and loss 0.5092635750770569\n",
      "epochs 885/400 and loss 0.5092633962631226\n",
      "epochs 886/400 and loss 0.5092653632164001\n",
      "epochs 887/400 and loss 0.5092654228210449\n",
      "epochs 888/400 and loss 0.509265124797821\n",
      "epochs 889/400 and loss 0.5092645287513733\n",
      "epochs 890/400 and loss 0.509263277053833\n",
      "epochs 891/400 and loss 0.5092665553092957\n",
      "epochs 892/400 and loss 0.5092689394950867\n",
      "epochs 893/400 and loss 0.5092650651931763\n",
      "epochs 894/400 and loss 0.5092651844024658\n",
      "epochs 895/400 and loss 0.509263277053833\n",
      "epochs 896/400 and loss 0.5092646479606628\n",
      "epochs 897/400 and loss 0.5092659592628479\n",
      "epochs 898/400 and loss 0.509265661239624\n",
      "epochs 899/400 and loss 0.5092639327049255\n",
      "epochs 900/400 and loss 0.5092664361000061\n",
      "epochs 901/400 and loss 0.5092648267745972\n",
      "epochs 902/400 and loss 0.5092677474021912\n",
      "epochs 903/400 and loss 0.5092628002166748\n",
      "epochs 904/400 and loss 0.5092676281929016\n",
      "epochs 905/400 and loss 0.5092636346817017\n",
      "epochs 906/400 and loss 0.5092634558677673\n",
      "epochs 907/400 and loss 0.5092653036117554\n",
      "epochs 908/400 and loss 0.5092645883560181\n",
      "epochs 909/400 and loss 0.5092641711235046\n",
      "epochs 910/400 and loss 0.5092649459838867\n",
      "epochs 911/400 and loss 0.5092634558677673\n",
      "epochs 912/400 and loss 0.5092662572860718\n",
      "epochs 913/400 and loss 0.5092675685882568\n",
      "epochs 914/400 and loss 0.5092664957046509\n",
      "epochs 915/400 and loss 0.5092668533325195\n",
      "epochs 916/400 and loss 0.5092657804489136\n",
      "epochs 917/400 and loss 0.5092666149139404\n",
      "epochs 918/400 and loss 0.5092663168907166\n",
      "epochs 919/400 and loss 0.5092629790306091\n",
      "epochs 920/400 and loss 0.5092650651931763\n",
      "epochs 921/400 and loss 0.5092634558677673\n",
      "epochs 922/400 and loss 0.5092640519142151\n",
      "epochs 923/400 and loss 0.5092644095420837\n",
      "epochs 924/400 and loss 0.5092676877975464\n",
      "epochs 925/400 and loss 0.509262204170227\n",
      "epochs 926/400 and loss 0.5092654228210449\n",
      "epochs 927/400 and loss 0.5092653632164001\n",
      "epochs 928/400 and loss 0.5092625617980957\n",
      "epochs 929/400 and loss 0.5092647075653076\n",
      "epochs 930/400 and loss 0.5092655420303345\n",
      "epochs 931/400 and loss 0.509263813495636\n",
      "epochs 932/400 and loss 0.5092678070068359\n",
      "epochs 933/400 and loss 0.5092636346817017\n",
      "epochs 934/400 and loss 0.5092629194259644\n",
      "epochs 935/400 and loss 0.5092661380767822\n",
      "epochs 936/400 and loss 0.5092653632164001\n",
      "epochs 937/400 and loss 0.5092636942863464\n",
      "epochs 938/400 and loss 0.509263277053833\n",
      "epochs 939/400 and loss 0.5092641115188599\n",
      "epochs 940/400 and loss 0.5092636346817017\n",
      "epochs 941/400 and loss 0.5092654228210449\n",
      "epochs 942/400 and loss 0.509264349937439\n",
      "epochs 943/400 and loss 0.5092669725418091\n",
      "epochs 944/400 and loss 0.5092624425888062\n",
      "epochs 945/400 and loss 0.5092667937278748\n",
      "epochs 946/400 and loss 0.509265124797821\n",
      "epochs 947/400 and loss 0.5092641115188599\n",
      "epochs 948/400 and loss 0.5092633366584778\n",
      "epochs 949/400 and loss 0.5092633366584778\n",
      "epochs 950/400 and loss 0.5092629194259644\n",
      "epochs 951/400 and loss 0.5092629194259644\n",
      "epochs 952/400 and loss 0.5092650651931763\n",
      "epochs 953/400 and loss 0.5092644691467285\n",
      "epochs 954/400 and loss 0.5092652440071106\n",
      "epochs 955/400 and loss 0.5092642903327942\n",
      "epochs 956/400 and loss 0.5092658400535583\n",
      "epochs 957/400 and loss 0.5092655420303345\n",
      "epochs 958/400 and loss 0.5092645287513733\n",
      "epochs 959/400 and loss 0.5092662572860718\n",
      "epochs 960/400 and loss 0.5092641711235046\n",
      "epochs 961/400 and loss 0.5092664957046509\n",
      "epochs 962/400 and loss 0.5092626810073853\n",
      "epochs 963/400 and loss 0.5092636346817017\n",
      "epochs 964/400 and loss 0.5092648863792419\n",
      "epochs 965/400 and loss 0.5092614889144897\n",
      "epochs 966/400 and loss 0.5092663168907166\n",
      "epochs 967/400 and loss 0.5092650055885315\n",
      "epochs 968/400 and loss 0.5092619061470032\n",
      "epochs 969/400 and loss 0.509260356426239\n",
      "epochs 970/400 and loss 0.5092633962631226\n",
      "epochs 971/400 and loss 0.5092658400535583\n",
      "epochs 972/400 and loss 0.5092624425888062\n",
      "epochs 973/400 and loss 0.5092635750770569\n",
      "epochs 974/400 and loss 0.509263813495636\n",
      "epochs 975/400 and loss 0.5092611908912659\n",
      "epochs 976/400 and loss 0.5092630982398987\n",
      "epochs 977/400 and loss 0.5092630386352539\n",
      "epochs 978/400 and loss 0.5092646479606628\n",
      "epochs 979/400 and loss 0.5092639923095703\n",
      "epochs 980/400 and loss 0.5092614889144897\n",
      "epochs 981/400 and loss 0.5092653036117554\n",
      "epochs 982/400 and loss 0.5092666745185852\n",
      "epochs 983/400 and loss 0.5092626810073853\n",
      "epochs 984/400 and loss 0.509262204170227\n",
      "epochs 985/400 and loss 0.5092626810073853\n",
      "epochs 986/400 and loss 0.5092639327049255\n",
      "epochs 987/400 and loss 0.5092645883560181\n",
      "epochs 988/400 and loss 0.5092635154724121\n",
      "epochs 989/400 and loss 0.5092645883560181\n",
      "epochs 990/400 and loss 0.5092617273330688\n",
      "epochs 991/400 and loss 0.509260356426239\n",
      "epochs 992/400 and loss 0.5092653036117554\n",
      "epochs 993/400 and loss 0.5092626214027405\n",
      "epochs 994/400 and loss 0.50926274061203\n",
      "epochs 995/400 and loss 0.5092639923095703\n",
      "epochs 996/400 and loss 0.509263277053833\n",
      "epochs 997/400 and loss 0.5092644095420837\n",
      "epochs 998/400 and loss 0.5092660784721375\n",
      "epochs 999/400 and loss 0.5092645287513733\n",
      "epochs 1000/400 and loss 0.5092625617980957\n",
      "epochs 1001/400 and loss 0.5092664957046509\n",
      "epochs 1002/400 and loss 0.509262204170227\n",
      "epochs 1003/400 and loss 0.5092644095420837\n",
      "epochs 1004/400 and loss 0.509262204170227\n",
      "epochs 1005/400 and loss 0.5092623829841614\n",
      "epochs 1006/400 and loss 0.5092620253562927\n",
      "epochs 1007/400 and loss 0.509262204170227\n",
      "epochs 1008/400 and loss 0.5092645287513733\n",
      "epochs 1009/400 and loss 0.5092628002166748\n",
      "epochs 1010/400 and loss 0.5092641115188599\n",
      "epochs 1011/400 and loss 0.5092621445655823\n",
      "epochs 1012/400 and loss 0.50926274061203\n",
      "epochs 1013/400 and loss 0.5092645883560181\n",
      "epochs 1014/400 and loss 0.5092626810073853\n",
      "epochs 1015/400 and loss 0.5092640519142151\n",
      "epochs 1016/400 and loss 0.5092613101005554\n",
      "epochs 1017/400 and loss 0.5092625617980957\n",
      "epochs 1018/400 and loss 0.50926274061203\n",
      "epochs 1019/400 and loss 0.5092618465423584\n",
      "epochs 1020/400 and loss 0.5092629194259644\n",
      "epochs 1021/400 and loss 0.5092641115188599\n",
      "epochs 1022/400 and loss 0.5092632174491882\n",
      "epochs 1023/400 and loss 0.5092639923095703\n",
      "epochs 1024/400 and loss 0.5092616677284241\n",
      "epochs 1025/400 and loss 0.509261965751648\n",
      "epochs 1026/400 and loss 0.5092639923095703\n",
      "epochs 1027/400 and loss 0.5092638731002808\n",
      "epochs 1028/400 and loss 0.50926274061203\n",
      "epochs 1029/400 and loss 0.5092630982398987\n",
      "epochs 1030/400 and loss 0.5092635750770569\n",
      "epochs 1031/400 and loss 0.5092629194259644\n",
      "epochs 1032/400 and loss 0.5092636346817017\n",
      "epochs 1033/400 and loss 0.5092631578445435\n",
      "epochs 1034/400 and loss 0.5092614889144897\n",
      "epochs 1035/400 and loss 0.5092641115188599\n",
      "epochs 1036/400 and loss 0.5092636346817017\n",
      "epochs 1037/400 and loss 0.509261429309845\n",
      "epochs 1038/400 and loss 0.509263277053833\n",
      "epochs 1039/400 and loss 0.5092635750770569\n",
      "epochs 1040/400 and loss 0.5092625021934509\n",
      "epochs 1041/400 and loss 0.5092629194259644\n",
      "epochs 1042/400 and loss 0.5092645287513733\n",
      "epochs 1043/400 and loss 0.5092611312866211\n",
      "epochs 1044/400 and loss 0.5092601180076599\n",
      "epochs 1045/400 and loss 0.5092628598213196\n",
      "epochs 1046/400 and loss 0.5092621445655823\n",
      "epochs 1047/400 and loss 0.5092620849609375\n",
      "epochs 1048/400 and loss 0.509262204170227\n",
      "epochs 1049/400 and loss 0.5092602968215942\n",
      "epochs 1050/400 and loss 0.5092633962631226\n",
      "epochs 1051/400 and loss 0.5092633366584778\n",
      "epochs 1052/400 and loss 0.5092606544494629\n",
      "epochs 1053/400 and loss 0.5092650651931763\n",
      "epochs 1054/400 and loss 0.5092641711235046\n",
      "epochs 1055/400 and loss 0.5092620849609375\n",
      "epochs 1056/400 and loss 0.5092605948448181\n",
      "epochs 1057/400 and loss 0.5092620849609375\n",
      "epochs 1058/400 and loss 0.5092653036117554\n",
      "epochs 1059/400 and loss 0.5092611312866211\n",
      "epochs 1060/400 and loss 0.5092620253562927\n",
      "epochs 1061/400 and loss 0.5092639327049255\n",
      "epochs 1062/400 and loss 0.5092639327049255\n",
      "epochs 1063/400 and loss 0.5092620253562927\n",
      "epochs 1064/400 and loss 0.5092605948448181\n",
      "epochs 1065/400 and loss 0.5092614889144897\n",
      "epochs 1066/400 and loss 0.509264349937439\n",
      "epochs 1067/400 and loss 0.5092612504959106\n",
      "epochs 1068/400 and loss 0.5092632174491882\n",
      "epochs 1069/400 and loss 0.5092647671699524\n",
      "epochs 1070/400 and loss 0.5092638731002808\n",
      "epochs 1071/400 and loss 0.5092640519142151\n",
      "epochs 1072/400 and loss 0.5092617273330688\n",
      "epochs 1073/400 and loss 0.5092641115188599\n",
      "epochs 1074/400 and loss 0.5092658400535583\n",
      "epochs 1075/400 and loss 0.5092593431472778\n",
      "epochs 1076/400 and loss 0.5092606544494629\n",
      "epochs 1077/400 and loss 0.509264349937439\n",
      "epochs 1078/400 and loss 0.509261965751648\n",
      "epochs 1079/400 and loss 0.5092612504959106\n",
      "epochs 1080/400 and loss 0.5092648267745972\n",
      "epochs 1081/400 and loss 0.5092602968215942\n",
      "epochs 1082/400 and loss 0.5092632174491882\n",
      "epochs 1083/400 and loss 0.5092635154724121\n",
      "epochs 1084/400 and loss 0.5092625617980957\n",
      "epochs 1085/400 and loss 0.509263813495636\n",
      "epochs 1086/400 and loss 0.5092639327049255\n",
      "epochs 1087/400 and loss 0.5092629790306091\n",
      "epochs 1088/400 and loss 0.5092631578445435\n",
      "epochs 1089/400 and loss 0.5092630982398987\n",
      "epochs 1090/400 and loss 0.5092638731002808\n",
      "epochs 1091/400 and loss 0.509262204170227\n",
      "epochs 1092/400 and loss 0.5092633962631226\n",
      "epochs 1093/400 and loss 0.5092658400535583\n",
      "epochs 1094/400 and loss 0.5092612504959106\n",
      "epochs 1095/400 and loss 0.5092628598213196\n",
      "epochs 1096/400 and loss 0.5092614889144897\n",
      "epochs 1097/400 and loss 0.5092610120773315\n",
      "epochs 1098/400 and loss 0.5092613697052002\n",
      "epochs 1099/400 and loss 0.5092637538909912\n",
      "epochs 1100/400 and loss 0.5092621445655823\n",
      "epochs 1101/400 and loss 0.5092645883560181\n",
      "epochs 1102/400 and loss 0.5092631578445435\n",
      "epochs 1103/400 and loss 0.5092633962631226\n",
      "epochs 1104/400 and loss 0.5092631578445435\n",
      "epochs 1105/400 and loss 0.5092625617980957\n",
      "epochs 1106/400 and loss 0.5092614889144897\n",
      "epochs 1107/400 and loss 0.5092610716819763\n",
      "epochs 1108/400 and loss 0.5092625617980957\n",
      "epochs 1109/400 and loss 0.5092655420303345\n",
      "epochs 1110/400 and loss 0.509260892868042\n",
      "epochs 1111/400 and loss 0.5092626810073853\n",
      "epochs 1112/400 and loss 0.5092630386352539\n",
      "epochs 1113/400 and loss 0.509260892868042\n",
      "epochs 1114/400 and loss 0.5092626810073853\n",
      "epochs 1115/400 and loss 0.5092613697052002\n",
      "epochs 1116/400 and loss 0.5092598795890808\n",
      "epochs 1117/400 and loss 0.5092631578445435\n",
      "epochs 1118/400 and loss 0.5092624425888062\n",
      "epochs 1119/400 and loss 0.5092624425888062\n",
      "epochs 1120/400 and loss 0.5092611312866211\n",
      "epochs 1121/400 and loss 0.509261965751648\n",
      "epochs 1122/400 and loss 0.5092635750770569\n",
      "epochs 1123/400 and loss 0.509262204170227\n",
      "epochs 1124/400 and loss 0.509262204170227\n",
      "epochs 1125/400 and loss 0.5092645883560181\n",
      "epochs 1126/400 and loss 0.5092620849609375\n",
      "epochs 1127/400 and loss 0.5092602372169495\n",
      "epochs 1128/400 and loss 0.5092649459838867\n",
      "epochs 1129/400 and loss 0.5092617869377136\n",
      "epochs 1130/400 and loss 0.5092623829841614\n",
      "epochs 1131/400 and loss 0.509260892868042\n",
      "epochs 1132/400 and loss 0.5092597603797913\n",
      "epochs 1133/400 and loss 0.5092636346817017\n",
      "epochs 1134/400 and loss 0.5092601180076599\n",
      "epochs 1135/400 and loss 0.5092626810073853\n",
      "epochs 1136/400 and loss 0.5092628002166748\n",
      "epochs 1137/400 and loss 0.5092617869377136\n",
      "epochs 1138/400 and loss 0.5092620849609375\n",
      "epochs 1139/400 and loss 0.5092618465423584\n",
      "epochs 1140/400 and loss 0.5092591047286987\n",
      "epochs 1141/400 and loss 0.5092648267745972\n",
      "epochs 1142/400 and loss 0.5092616677284241\n",
      "epochs 1143/400 and loss 0.5092637538909912\n",
      "epochs 1144/400 and loss 0.5092635154724121\n",
      "epochs 1145/400 and loss 0.5092600584030151\n",
      "epochs 1146/400 and loss 0.5092628002166748\n",
      "epochs 1147/400 and loss 0.5092635750770569\n",
      "epochs 1148/400 and loss 0.5092594623565674\n",
      "epochs 1149/400 and loss 0.5092605948448181\n",
      "epochs 1150/400 and loss 0.5092628598213196\n",
      "epochs 1151/400 and loss 0.5092614889144897\n",
      "epochs 1152/400 and loss 0.5092605352401733\n",
      "epochs 1153/400 and loss 0.5092595815658569\n",
      "epochs 1154/400 and loss 0.5092631578445435\n",
      "epochs 1155/400 and loss 0.5092607736587524\n",
      "epochs 1156/400 and loss 0.5092605352401733\n",
      "epochs 1157/400 and loss 0.5092630386352539\n",
      "epochs 1158/400 and loss 0.5092626810073853\n",
      "epochs 1159/400 and loss 0.5092602372169495\n",
      "epochs 1160/400 and loss 0.509261429309845\n",
      "epochs 1161/400 and loss 0.5092608332633972\n",
      "epochs 1162/400 and loss 0.5092617869377136\n",
      "epochs 1163/400 and loss 0.5092641115188599\n",
      "epochs 1164/400 and loss 0.5092607140541077\n",
      "epochs 1165/400 and loss 0.5092611312866211\n",
      "epochs 1166/400 and loss 0.5092630982398987\n",
      "epochs 1167/400 and loss 0.5092636346817017\n",
      "epochs 1168/400 and loss 0.5092597007751465\n",
      "epochs 1169/400 and loss 0.5092609524726868\n",
      "epochs 1170/400 and loss 0.5092624425888062\n",
      "epochs 1171/400 and loss 0.509259819984436\n",
      "epochs 1172/400 and loss 0.5092597007751465\n",
      "epochs 1173/400 and loss 0.509261965751648\n",
      "epochs 1174/400 and loss 0.5092628598213196\n",
      "epochs 1175/400 and loss 0.5092605352401733\n",
      "epochs 1176/400 and loss 0.509259045124054\n",
      "epochs 1177/400 and loss 0.5092587471008301\n",
      "epochs 1178/400 and loss 0.5092625021934509\n",
      "epochs 1179/400 and loss 0.509261965751648\n",
      "epochs 1180/400 and loss 0.5092597007751465\n",
      "epochs 1181/400 and loss 0.5092613101005554\n",
      "epochs 1182/400 and loss 0.5092597007751465\n",
      "epochs 1183/400 and loss 0.5092588663101196\n",
      "epochs 1184/400 and loss 0.509262204170227\n",
      "epochs 1185/400 and loss 0.5092614889144897\n",
      "epochs 1186/400 and loss 0.5092630386352539\n",
      "epochs 1187/400 and loss 0.5092599987983704\n",
      "epochs 1188/400 and loss 0.5092605352401733\n",
      "epochs 1189/400 and loss 0.5092611312866211\n",
      "epochs 1190/400 and loss 0.5092626810073853\n",
      "epochs 1191/400 and loss 0.5092601180076599\n",
      "epochs 1192/400 and loss 0.5092600584030151\n",
      "epochs 1193/400 and loss 0.5092601776123047\n",
      "epochs 1194/400 and loss 0.5092620849609375\n",
      "epochs 1195/400 and loss 0.5092605352401733\n",
      "epochs 1196/400 and loss 0.5092586278915405\n",
      "epochs 1197/400 and loss 0.5092597603797913\n",
      "epochs 1198/400 and loss 0.509261429309845\n",
      "epochs 1199/400 and loss 0.5092639923095703\n",
      "epochs 1200/400 and loss 0.5092574954032898\n",
      "epochs 1201/400 and loss 0.5092597007751465\n",
      "epochs 1202/400 and loss 0.5092636346817017\n",
      "epochs 1203/400 and loss 0.5092610120773315\n",
      "epochs 1204/400 and loss 0.5092616677284241\n",
      "epochs 1205/400 and loss 0.5092624425888062\n",
      "epochs 1206/400 and loss 0.5092600584030151\n",
      "epochs 1207/400 and loss 0.5092602968215942\n",
      "epochs 1208/400 and loss 0.5092611908912659\n",
      "epochs 1209/400 and loss 0.5092595815658569\n",
      "epochs 1210/400 and loss 0.509263277053833\n",
      "epochs 1211/400 and loss 0.5092623233795166\n",
      "epochs 1212/400 and loss 0.5092577338218689\n",
      "epochs 1213/400 and loss 0.5092620849609375\n",
      "epochs 1214/400 and loss 0.5092637538909912\n",
      "epochs 1215/400 and loss 0.5092599391937256\n",
      "epochs 1216/400 and loss 0.5092613101005554\n",
      "epochs 1217/400 and loss 0.5092611312866211\n",
      "epochs 1218/400 and loss 0.5092599391937256\n",
      "epochs 1219/400 and loss 0.5092591643333435\n",
      "epochs 1220/400 and loss 0.5092591643333435\n",
      "epochs 1221/400 and loss 0.5092636942863464\n",
      "epochs 1222/400 and loss 0.5092610120773315\n",
      "epochs 1223/400 and loss 0.5092620253562927\n",
      "epochs 1224/400 and loss 0.5092601776123047\n",
      "epochs 1225/400 and loss 0.5092607140541077\n",
      "epochs 1226/400 and loss 0.5092620849609375\n",
      "epochs 1227/400 and loss 0.5092620849609375\n",
      "epochs 1228/400 and loss 0.5092579126358032\n",
      "epochs 1229/400 and loss 0.5092617869377136\n",
      "epochs 1230/400 and loss 0.5092623829841614\n",
      "epochs 1231/400 and loss 0.5092635154724121\n",
      "epochs 1232/400 and loss 0.5092593431472778\n",
      "epochs 1233/400 and loss 0.5092594623565674\n",
      "epochs 1234/400 and loss 0.5092616081237793\n",
      "epochs 1235/400 and loss 0.5092588663101196\n",
      "epochs 1236/400 and loss 0.5092599391937256\n",
      "epochs 1237/400 and loss 0.5092613101005554\n",
      "epochs 1238/400 and loss 0.5092595815658569\n",
      "epochs 1239/400 and loss 0.5092597007751465\n",
      "epochs 1240/400 and loss 0.5092596411705017\n",
      "epochs 1241/400 and loss 0.5092591047286987\n",
      "epochs 1242/400 and loss 0.5092629790306091\n",
      "epochs 1243/400 and loss 0.5092623233795166\n",
      "epochs 1244/400 and loss 0.509259819984436\n",
      "epochs 1245/400 and loss 0.5092620849609375\n",
      "epochs 1246/400 and loss 0.5092622637748718\n",
      "epochs 1247/400 and loss 0.5092582702636719\n",
      "epochs 1248/400 and loss 0.5092591047286987\n",
      "epochs 1249/400 and loss 0.5092600584030151\n",
      "epochs 1250/400 and loss 0.5092613101005554\n",
      "epochs 1251/400 and loss 0.5092595815658569\n",
      "epochs 1252/400 and loss 0.5092619061470032\n",
      "epochs 1253/400 and loss 0.5092601180076599\n",
      "epochs 1254/400 and loss 0.5092610120773315\n",
      "epochs 1255/400 and loss 0.5092605352401733\n",
      "epochs 1256/400 and loss 0.5092600584030151\n",
      "epochs 1257/400 and loss 0.5092605352401733\n",
      "epochs 1258/400 and loss 0.5092626810073853\n",
      "epochs 1259/400 and loss 0.5092612504959106\n",
      "epochs 1260/400 and loss 0.5092576742172241\n",
      "epochs 1261/400 and loss 0.5092621445655823\n",
      "epochs 1262/400 and loss 0.509262204170227\n",
      "epochs 1263/400 and loss 0.5092631578445435\n",
      "epochs 1264/400 and loss 0.5092613101005554\n",
      "epochs 1265/400 and loss 0.5092599987983704\n",
      "epochs 1266/400 and loss 0.5092574954032898\n",
      "epochs 1267/400 and loss 0.5092607140541077\n",
      "epochs 1268/400 and loss 0.5092600584030151\n",
      "epochs 1269/400 and loss 0.5092594623565674\n",
      "epochs 1270/400 and loss 0.5092607736587524\n",
      "epochs 1271/400 and loss 0.5092617869377136\n",
      "epochs 1272/400 and loss 0.5092605352401733\n",
      "epochs 1273/400 and loss 0.5092609524726868\n",
      "epochs 1274/400 and loss 0.5092610716819763\n",
      "epochs 1275/400 and loss 0.5092602372169495\n",
      "epochs 1276/400 and loss 0.5092555284500122\n",
      "epochs 1277/400 and loss 0.5092598795890808\n",
      "epochs 1278/400 and loss 0.5092625617980957\n",
      "epochs 1279/400 and loss 0.5092595219612122\n",
      "epochs 1280/400 and loss 0.5092602372169495\n",
      "epochs 1281/400 and loss 0.5092613697052002\n",
      "epochs 1282/400 and loss 0.509257972240448\n",
      "epochs 1283/400 and loss 0.5092591047286987\n",
      "epochs 1284/400 and loss 0.5092605352401733\n",
      "epochs 1285/400 and loss 0.509258508682251\n",
      "epochs 1286/400 and loss 0.509262204170227\n",
      "epochs 1287/400 and loss 0.5092604756355286\n",
      "epochs 1288/400 and loss 0.5092588067054749\n",
      "epochs 1289/400 and loss 0.5092606544494629\n",
      "epochs 1290/400 and loss 0.5092639923095703\n",
      "epochs 1291/400 and loss 0.5092584490776062\n",
      "epochs 1292/400 and loss 0.5092606544494629\n",
      "epochs 1293/400 and loss 0.5092604160308838\n",
      "epochs 1294/400 and loss 0.509261965751648\n",
      "epochs 1295/400 and loss 0.5092600584030151\n",
      "epochs 1296/400 and loss 0.5092588663101196\n",
      "epochs 1297/400 and loss 0.5092609524726868\n",
      "epochs 1298/400 and loss 0.5092588067054749\n",
      "epochs 1299/400 and loss 0.5092599391937256\n",
      "epochs 1300/400 and loss 0.5092614889144897\n",
      "epochs 1301/400 and loss 0.5092600584030151\n",
      "epochs 1302/400 and loss 0.5092588067054749\n",
      "epochs 1303/400 and loss 0.5092614889144897\n",
      "epochs 1304/400 and loss 0.5092594623565674\n",
      "epochs 1305/400 and loss 0.5092607736587524\n",
      "epochs 1306/400 and loss 0.509259819984436\n",
      "epochs 1307/400 and loss 0.5092620253562927\n",
      "epochs 1308/400 and loss 0.5092575550079346\n",
      "epochs 1309/400 and loss 0.5092579126358032\n",
      "epochs 1310/400 and loss 0.5092630982398987\n",
      "epochs 1311/400 and loss 0.5092618465423584\n",
      "epochs 1312/400 and loss 0.5092588663101196\n",
      "epochs 1313/400 and loss 0.5092591643333435\n",
      "epochs 1314/400 and loss 0.5092599987983704\n",
      "epochs 1315/400 and loss 0.5092569589614868\n",
      "epochs 1316/400 and loss 0.5092582106590271\n",
      "epochs 1317/400 and loss 0.5092591047286987\n",
      "epochs 1318/400 and loss 0.5092600584030151\n",
      "epochs 1319/400 and loss 0.5092589259147644\n",
      "epochs 1320/400 and loss 0.509260356426239\n",
      "epochs 1321/400 and loss 0.5092605352401733\n",
      "epochs 1322/400 and loss 0.5092612504959106\n",
      "epochs 1323/400 and loss 0.5092583894729614\n",
      "epochs 1324/400 and loss 0.509257435798645\n",
      "epochs 1325/400 and loss 0.5092573761940002\n",
      "epochs 1326/400 and loss 0.5092616081237793\n",
      "epochs 1327/400 and loss 0.5092625021934509\n",
      "epochs 1328/400 and loss 0.5092588663101196\n",
      "epochs 1329/400 and loss 0.5092602372169495\n",
      "epochs 1330/400 and loss 0.5092582106590271\n",
      "epochs 1331/400 and loss 0.5092607140541077\n",
      "epochs 1332/400 and loss 0.509259819984436\n",
      "epochs 1333/400 and loss 0.5092593431472778\n",
      "epochs 1334/400 and loss 0.5092591643333435\n",
      "epochs 1335/400 and loss 0.509257435798645\n",
      "epochs 1336/400 and loss 0.5092579126358032\n",
      "epochs 1337/400 and loss 0.5092600584030151\n",
      "epochs 1338/400 and loss 0.5092591047286987\n",
      "epochs 1339/400 and loss 0.5092600584030151\n",
      "epochs 1340/400 and loss 0.5092558264732361\n",
      "epochs 1341/400 and loss 0.509260892868042\n",
      "epochs 1342/400 and loss 0.5092605948448181\n",
      "epochs 1343/400 and loss 0.5092609524726868\n",
      "epochs 1344/400 and loss 0.5092614889144897\n",
      "epochs 1345/400 and loss 0.5092592239379883\n",
      "epochs 1346/400 and loss 0.5092576742172241\n",
      "epochs 1347/400 and loss 0.5092620849609375\n",
      "epochs 1348/400 and loss 0.5092614889144897\n",
      "epochs 1349/400 and loss 0.5092610120773315\n",
      "epochs 1350/400 and loss 0.5092593431472778\n",
      "epochs 1351/400 and loss 0.509262204170227\n",
      "epochs 1352/400 and loss 0.5092610120773315\n",
      "epochs 1353/400 and loss 0.5092602372169495\n",
      "epochs 1354/400 and loss 0.5092597007751465\n",
      "epochs 1355/400 and loss 0.5092594623565674\n",
      "epochs 1356/400 and loss 0.5092569589614868\n",
      "epochs 1357/400 and loss 0.5092602968215942\n",
      "epochs 1358/400 and loss 0.509257972240448\n",
      "epochs 1359/400 and loss 0.5092599987983704\n",
      "epochs 1360/400 and loss 0.5092583298683167\n",
      "epochs 1361/400 and loss 0.509257435798645\n",
      "epochs 1362/400 and loss 0.5092605352401733\n",
      "epochs 1363/400 and loss 0.509257972240448\n",
      "epochs 1364/400 and loss 0.5092569589614868\n",
      "epochs 1365/400 and loss 0.5092591047286987\n",
      "epochs 1366/400 and loss 0.5092564821243286\n",
      "epochs 1367/400 and loss 0.5092588663101196\n",
      "epochs 1368/400 and loss 0.5092601180076599\n",
      "epochs 1369/400 and loss 0.5092591047286987\n",
      "epochs 1370/400 and loss 0.5092597007751465\n",
      "epochs 1371/400 and loss 0.5092601776123047\n",
      "epochs 1372/400 and loss 0.5092601776123047\n",
      "epochs 1373/400 and loss 0.5092602372169495\n",
      "epochs 1374/400 and loss 0.5092610120773315\n",
      "epochs 1375/400 and loss 0.5092587471008301\n",
      "epochs 1376/400 and loss 0.5092588663101196\n",
      "epochs 1377/400 and loss 0.5092569589614868\n",
      "epochs 1378/400 and loss 0.5092574954032898\n",
      "epochs 1379/400 and loss 0.5092591047286987\n",
      "epochs 1380/400 and loss 0.509256899356842\n",
      "epochs 1381/400 and loss 0.509260892868042\n",
      "epochs 1382/400 and loss 0.5092595815658569\n",
      "epochs 1383/400 and loss 0.5092617869377136\n",
      "epochs 1384/400 and loss 0.5092602968215942\n",
      "epochs 1385/400 and loss 0.5092589855194092\n",
      "epochs 1386/400 and loss 0.5092581510543823\n",
      "epochs 1387/400 and loss 0.5092595815658569\n",
      "epochs 1388/400 and loss 0.509259045124054\n",
      "epochs 1389/400 and loss 0.5092611312866211\n",
      "epochs 1390/400 and loss 0.5092581510543823\n",
      "epochs 1391/400 and loss 0.5092591047286987\n",
      "epochs 1392/400 and loss 0.5092616081237793\n",
      "epochs 1393/400 and loss 0.5092597007751465\n",
      "epochs 1394/400 and loss 0.5092571377754211\n",
      "epochs 1395/400 and loss 0.509257972240448\n",
      "epochs 1396/400 and loss 0.5092562437057495\n",
      "epochs 1397/400 and loss 0.5092578530311584\n",
      "epochs 1398/400 and loss 0.5092577934265137\n",
      "epochs 1399/400 and loss 0.5092605948448181\n",
      "epochs 1400/400 and loss 0.5092601180076599\n",
      "epochs 1401/400 and loss 0.5092586278915405\n",
      "epochs 1402/400 and loss 0.5092565417289734\n",
      "epochs 1403/400 and loss 0.5092605948448181\n",
      "epochs 1404/400 and loss 0.5092606544494629\n",
      "epochs 1405/400 and loss 0.5092571377754211\n",
      "epochs 1406/400 and loss 0.5092594623565674\n",
      "epochs 1407/400 and loss 0.5092579126358032\n",
      "epochs 1408/400 and loss 0.5092600584030151\n",
      "epochs 1409/400 and loss 0.5092602968215942\n",
      "epochs 1410/400 and loss 0.5092576742172241\n",
      "epochs 1411/400 and loss 0.5092573761940002\n",
      "epochs 1412/400 and loss 0.509260356426239\n",
      "epochs 1413/400 and loss 0.5092564821243286\n",
      "epochs 1414/400 and loss 0.5092613101005554\n",
      "epochs 1415/400 and loss 0.5092594623565674\n",
      "epochs 1416/400 and loss 0.5092573761940002\n",
      "epochs 1417/400 and loss 0.5092585682868958\n",
      "epochs 1418/400 and loss 0.5092583894729614\n",
      "epochs 1419/400 and loss 0.5092609524726868\n",
      "epochs 1420/400 and loss 0.5092588663101196\n",
      "epochs 1421/400 and loss 0.5092585682868958\n",
      "epochs 1422/400 and loss 0.5092587471008301\n",
      "epochs 1423/400 and loss 0.5092597603797913\n",
      "epochs 1424/400 and loss 0.5092610120773315\n",
      "epochs 1425/400 and loss 0.5092583298683167\n",
      "epochs 1426/400 and loss 0.5092599987983704\n",
      "epochs 1427/400 and loss 0.5092567801475525\n",
      "epochs 1428/400 and loss 0.5092567205429077\n",
      "epochs 1429/400 and loss 0.5092602968215942\n",
      "epochs 1430/400 and loss 0.5092596411705017\n",
      "epochs 1431/400 and loss 0.5092591047286987\n",
      "epochs 1432/400 and loss 0.5092573165893555\n",
      "epochs 1433/400 and loss 0.5092589259147644\n",
      "epochs 1434/400 and loss 0.5092586278915405\n",
      "epochs 1435/400 and loss 0.5092599391937256\n",
      "epochs 1436/400 and loss 0.5092588663101196\n",
      "epochs 1437/400 and loss 0.5092592835426331\n",
      "epochs 1438/400 and loss 0.5092611908912659\n",
      "epochs 1439/400 and loss 0.5092554092407227\n",
      "epochs 1440/400 and loss 0.5092588663101196\n",
      "epochs 1441/400 and loss 0.5092606544494629\n",
      "epochs 1442/400 and loss 0.5092563033103943\n",
      "epochs 1443/400 and loss 0.5092568397521973\n",
      "epochs 1444/400 and loss 0.509259819984436\n",
      "epochs 1445/400 and loss 0.5092613101005554\n",
      "epochs 1446/400 and loss 0.5092574954032898\n",
      "epochs 1447/400 and loss 0.50925612449646\n",
      "epochs 1448/400 and loss 0.5092582106590271\n",
      "epochs 1449/400 and loss 0.509257972240448\n",
      "epochs 1450/400 and loss 0.5092579126358032\n",
      "epochs 1451/400 and loss 0.5092588663101196\n",
      "epochs 1452/400 and loss 0.5092588067054749\n",
      "epochs 1453/400 and loss 0.5092589855194092\n",
      "epochs 1454/400 and loss 0.5092614889144897\n",
      "epochs 1455/400 and loss 0.5092604160308838\n",
      "epochs 1456/400 and loss 0.5092610716819763\n",
      "epochs 1457/400 and loss 0.5092586278915405\n",
      "epochs 1458/400 and loss 0.5092582106590271\n",
      "epochs 1459/400 and loss 0.5092576742172241\n",
      "epochs 1460/400 and loss 0.5092589855194092\n",
      "epochs 1461/400 and loss 0.5092592239379883\n",
      "epochs 1462/400 and loss 0.5092569589614868\n",
      "epochs 1463/400 and loss 0.5092573761940002\n",
      "epochs 1464/400 and loss 0.5092591047286987\n",
      "epochs 1465/400 and loss 0.5092593431472778\n",
      "epochs 1466/400 and loss 0.5092581510543823\n",
      "epochs 1467/400 and loss 0.5092594027519226\n",
      "epochs 1468/400 and loss 0.5092607736587524\n",
      "epochs 1469/400 and loss 0.5092580318450928\n",
      "epochs 1470/400 and loss 0.5092602968215942\n",
      "epochs 1471/400 and loss 0.5092566013336182\n",
      "epochs 1472/400 and loss 0.5092595815658569\n",
      "epochs 1473/400 and loss 0.5092557072639465\n",
      "epochs 1474/400 and loss 0.5092564821243286\n",
      "epochs 1475/400 and loss 0.5092606544494629\n",
      "epochs 1476/400 and loss 0.5092565417289734\n",
      "epochs 1477/400 and loss 0.509257972240448\n",
      "epochs 1478/400 and loss 0.5092593431472778\n",
      "epochs 1479/400 and loss 0.509258508682251\n",
      "epochs 1480/400 and loss 0.5092562437057495\n",
      "epochs 1481/400 and loss 0.5092582702636719\n",
      "epochs 1482/400 and loss 0.5092580914497375\n",
      "epochs 1483/400 and loss 0.5092606544494629\n",
      "epochs 1484/400 and loss 0.5092571973800659\n",
      "epochs 1485/400 and loss 0.5092546343803406\n",
      "epochs 1486/400 and loss 0.5092592239379883\n",
      "epochs 1487/400 and loss 0.5092605352401733\n",
      "epochs 1488/400 and loss 0.5092570185661316\n",
      "epochs 1489/400 and loss 0.5092565417289734\n",
      "epochs 1490/400 and loss 0.509256899356842\n",
      "epochs 1491/400 and loss 0.5092570781707764\n",
      "epochs 1492/400 and loss 0.509257435798645\n",
      "epochs 1493/400 and loss 0.5092593431472778\n",
      "epochs 1494/400 and loss 0.5092583298683167\n",
      "epochs 1495/400 and loss 0.5092587471008301\n",
      "epochs 1496/400 and loss 0.5092579126358032\n",
      "epochs 1497/400 and loss 0.5092576742172241\n",
      "epochs 1498/400 and loss 0.5092583894729614\n",
      "epochs 1499/400 and loss 0.5092567801475525\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i  in range(1500):\n",
    "    preds = model(inputs)\n",
    "    loss = MSE(preds,targets)\n",
    "    losses.append(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f'epochs {i}/400 and loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5093, 0.5093, 0.5093,  ..., 0.5093, 0.5093, 0.5093])\n"
     ]
    }
   ],
   "source": [
    "normal_loss = torch.tensor(losses)\n",
    "print(normal_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x218ce917560>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(normal_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.TensorDataset"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
