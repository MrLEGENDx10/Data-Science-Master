{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "### Performance Metrics - Accuracy, Precision, Recall, F1 Score, ROC-AUC Score\n",
    "### Regularization - L1, L2\n",
    "### Hyperparameter Tuning - GridSearchCV , RandomizedSearchCV\n",
    "### Cross Validation - KFold, StratifiedKFold\n",
    "### Logistic Regression Multiclass Classification - One vs Rest, One vs One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Introduction\n",
    "New to machine learning? Explore Logistic Regression with us, a beginner-friendly approach to predictive modeling. We’ll break down the basics, show you practical uses, and make it easy for you to apply in real-life situations. Let’s learn together!\n",
    "\n",
    "Well, these were a few of my doubts when I was learning Logistic Regression. To find the math behind this, I plunged deeper into this topic only to find myself a better understanding of the Logistic Regression model. And in this article, I will try to answer all the doubts you are having right now on this topic. I will tell you the math behind this regression model.\n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [What is Logistic Regression?](#what-is-logistic-regression)\n",
    "3. [Types of Logistic Regression](#types-of-logistic-regression)\n",
    "4. [Why do we use Logistic Regression rather than Linear Regression?](#why-use-logistic-regression)\n",
    "5. [How does Logistic Regression work?](#how-logistic-regression-works)\n",
    "    - [Logistic Function](#logistic-function)\n",
    "    - [Cost Function in Logistic Regression](#cost-function)\n",
    "    - [What is the use of Maximum Likelihood Estimator?](#maximum-likelihood-estimator)\n",
    "    - [Gradient Descent Optimization](#gradient-descent-optimization)\n",
    "\n",
    "[Visit for more information](https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall in Machine Learning\n",
    "\n",
    "### Introduction\n",
    "Ask any machine learning, data science professional, or data scientist about the most confusing concepts in their learning journey. And invariably, the answer veers towards Precision and Recall. The difference between Precision and Recall is actually easy to remember – but only once you’ve truly understood what each term stands for. But quite often, and I can attest to this, experts tend to offer half-baked explanations which confuse newcomers even more.\n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Precision and Recall Trade-off](#precision-and-recall-trade-off)\n",
    "3. [Understanding the Problem Statement](#understanding-the-problem-statement)\n",
    "4. [What Is a Confusion Matrix?](#what-is-a-confusion-matrix)\n",
    "5. [What Is Precision?](#what-is-precision)\n",
    "6. [What Is Recall?](#what-is-recall)\n",
    "7. [What Is Accuracy Metric?](#what-is-accuracy-metric)\n",
    "8. [The Role of the F1-Score](#the-role-of-the-f1-score)\n",
    "9. [False Positive Rate & True Negative Rate](#false-positive-rate-true-negative-rate)\n",
    "10. [Receiver Operating Characteristic Curve (ROC Curve)](#roc-curve)\n",
    "11. [Precision-Recall Curve (PRC)](#precision-recall-curve)\n",
    "12. [Conclusion](#conclusion)\n",
    "\n",
    "[Visit for more information](https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Regularization\n",
    "1. Introduction to Logistic Regression:\n",
    "Logistic regression is a statistical method used for binary classification problems.\n",
    "It models the probability of a binary outcome based on one or more predictor variables.\n",
    "\n",
    "2. Need for Regularization:\n",
    "In machine learning, overfitting occurs when a model learns to fit the training data too closely, capturing noise rather than underlying patterns.\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function.\n",
    "\n",
    "3. Types of Regularization:\n",
    "L1 Regularization (Lasso):\n",
    "Adds the sum of the absolute values of the coefficients as a penalty term.\n",
    "Encourages sparsity by shrinking some coefficients to exactly zero, effectively performing feature selection.\n",
    "Particularly useful when dealing with high-dimensional data where feature selection is essential.\n",
    "L2 Regularization (Ridge):\n",
    "Adds the sum of the squared values of the coefficients as a penalty term.\n",
    "Doesn't enforce sparsity but penalizes large coefficients, effectively discouraging complex models.\n",
    "Useful for preventing multicollinearity and stabilizing the coefficients.\n",
    "\n",
    "4. Implementation in Logistic Regression:\n",
    "Scikit-Learn in Python:\n",
    "LogisticRegression class from the sklearn.linear_model module.\n",
    "Parameters:\n",
    "penalty: Specifies the type of regularization ('l1' for L1, 'l2' for L2).\n",
    "C: Regularization strength parameter (inverse of regularization strength; smaller values indicate stronger regularization).\n",
    "solver: Algorithm to use in the optimization problem ('liblinear' for L1, 'lbfgs' for L2).\n",
    "Example:\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l1', C=1.0, solver='liblinear')\n",
    "```\n",
    "\n",
    "5. Tuning Regularization Strength (C):\n",
    "The choice of the regularization strength parameter, denoted as C, influences the balance between bias and variance.\n",
    "Cross-validation techniques (e.g., grid search, randomized search) can be used to find the optimal value of C.\n",
    "\n",
    "6. Performance Evaluation:\n",
    "After training the regularized logistic regression model, performance evaluation metrics such as accuracy, precision, recall, F1 score, and ROC-AUC can be used to assess its effectiveness.\n",
    "\n",
    "7. Conclusion:\n",
    "Regularization is a crucial technique in logistic regression to prevent overfitting and improve model generalization.\n",
    "By incorporating either L1 (Lasso) or L2 (Ridge) regularization, the model's complexity can be controlled, leading to better predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used to prevent overfitting in machine learning models, including logistic regression. Overfitting occurs when a model learns the training data too well, capturing noise and outliers in addition to the underlying patterns. As a result, it performs poorly on unseen data.\n",
    "\n",
    "In logistic regression, the goal is to find the best parameters (weights) for the features in order to predict the target variable. Regularization introduces a penalty term to the loss function that the algorithm optimizes, discouraging complex models by making the coefficients smaller.\n",
    "\n",
    "There are two main types of regularization:\n",
    "\n",
    "1. **L1 regularization (Lasso regression)**: Adds a penalty equal to the absolute value of the magnitude of coefficients. This can result in some coefficients being zero, effectively performing feature selection.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model with L1 regularization\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "```\n",
    "\n",
    "2. **L2 regularization (Ridge regression)**: Adds a penalty equal to the square of the magnitude of coefficients. This tends to result in smaller coefficients overall, but it doesn't force them to zero.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model with L2 regularization\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "```\n",
    "\n",
    "In both cases, the strength of the regularization is controlled by the hyperparameter `C`, which is the inverse of the regularization strength. A smaller `C` specifies stronger regularization.\n",
    "\n",
    "```python\n",
    "# Create a logistic regression model with L2 regularization and a specific C value\n",
    "model = LogisticRegression(penalty='l2', C=0.1, solver='liblinear')\n",
    "```\n",
    "\n",
    "Remember to always scale your data before applying regularization, as it is sensitive to the scale of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/05/4-ways-to-evaluate-your-machine-learning-model-cross-validation-techniques-with-python-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2022/02/k-fold-cross-validation-technique-and-its-essentials/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
